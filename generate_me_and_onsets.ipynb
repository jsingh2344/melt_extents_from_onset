{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5437ce28-f3bc-49ea-882d-72334f3f9a83",
   "metadata": {},
   "source": [
    "# SAR Melt Extent Processing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf3ed2-f031-47f9-a4a7-01c7914d5ffb",
   "metadata": {},
   "source": [
    "The notebook does the following:\n",
    "- processes the individual glaciers within the scene to get the melt extents using a \"percentile\" method:\n",
    "    - DEM elevation aggregation is done after melt extents are determined per pixel. Aggregation is done based on the glacier hypsometry.\n",
    "\n",
    "Authors: David Rounce (CMU) and Mark Fahnestock (UAF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff072da0-3eb9-4dbe-9f52-08972cacbfe8",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a65277ed-79f9-4f5c-81f3-e4061aae7b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and function definitions\n",
    "import glob, os, re\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from datetime import date, timedelta\n",
    "from scipy.ndimage import convolve\n",
    "from rasterio.transform import Affine\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2df3557-5c13-4f99-938b-fec9f0174323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de5a7bed-1cf0-4c81-be5e-4b43586a9f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faster alternative to np.nanpercentile -- from https://krstn.eu/np.nanpercentile()-there-has-to-be-a-faster-way/\n",
    "def nan_percentile(arr, q, axis=0):\n",
    "    arr = np.asarray(arr)\n",
    "    # Move the desired axis to the front\n",
    "    arr = np.moveaxis(arr, axis, 0)\n",
    "\n",
    "    # valid (non-NaN) observations along the first axis\n",
    "    valid_obs = np.sum(np.isfinite(arr), axis=0)\n",
    "    max_val = np.nanmax(arr)\n",
    "    arr = arr.copy()  # avoid modifying original input\n",
    "    arr[np.isnan(arr)] = max_val\n",
    "    arr = np.sort(arr, axis=0)\n",
    "\n",
    "    # Handle list or single q\n",
    "    qs = [q] if np.isscalar(q) else q\n",
    "    result = []\n",
    "\n",
    "    for quant in qs:\n",
    "        k_arr = (valid_obs - 1) * (quant / 100.0)\n",
    "        f_arr = np.floor(k_arr).astype(np.int32)\n",
    "        c_arr = np.ceil(k_arr).astype(np.int32)\n",
    "        fc_equal_k_mask = f_arr == c_arr\n",
    "\n",
    "        floor_val = _zvalue_from_index(arr, f_arr) * (c_arr - k_arr)\n",
    "        ceil_val  = _zvalue_from_index(arr, c_arr) * (k_arr - f_arr)\n",
    "\n",
    "        quant_arr = floor_val + ceil_val\n",
    "        quant_arr[fc_equal_k_mask] = _zvalue_from_index(arr, k_arr.astype(np.int32))[fc_equal_k_mask]\n",
    "\n",
    "        result.append(quant_arr)\n",
    "\n",
    "    if np.isscalar(q):\n",
    "        return result[0]\n",
    "    return np.stack(result, axis=0)\n",
    "\n",
    "def _zvalue_from_index(arr, ind):\n",
    "    \"\"\"\n",
    "    Extracts values along the first axis of a 3D array given 2D indices.\n",
    "    \"\"\"\n",
    "    # arr shape = (depth, y, x)\n",
    "    d, nC, nR = arr.shape\n",
    "    # Compute linear indices\n",
    "    idx = nC*nR*ind + nR*np.arange(nC)[:, None] + np.arange(nR)\n",
    "    return np.take(arr, idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b4164b-1dfb-4dfb-a27a-fb0bf3db8ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescStr:\n",
    "    def __init__(self):\n",
    "        self._desc = ''\n",
    "    def write(self, instr):\n",
    "        self._desc += re.sub('\\n|\\x1b.*|\\r', '', instr)\n",
    "    def read(self):\n",
    "        ret = self._desc\n",
    "        self._desc = ''\n",
    "        return ret\n",
    "    def flush(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6c27d-fa73-497c-9f07-a863d5e08841",
   "metadata": {},
   "source": [
    "# Find the datacube\n",
    "Currently this must be done separately for each path/row combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814b233f-2505-4745-88a6-5e6bc00d86ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  \n",
    "# set up job parameters\n",
    "# \n",
    "# Set up paths/bbox\n",
    "out_nc = Path('/Users/albinwells/Desktop/SAR Alaska processing/for_yujun_alicia/SAR_preprocess_out/') # netcdf output\n",
    "\n",
    "main_directory = os.getcwd()\n",
    "fig_fp = 'figures'\n",
    "if not os.path.exists(fig_fp):\n",
    "    os.makedirs(fig_fp)\n",
    "csv_fp = 'csv'\n",
    "if not os.path.exists(csv_fp):\n",
    "    os.makedirs(csv_fp)\n",
    "\n",
    "# path_frame pairs to process - dirs on same path will be searched for same-date images to be read together\n",
    "# firs key is path, which returns a list of frames for that path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e740d4-a557-4bc7-b5cd-d74f962faae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define glacier outline\n",
    "RGI_shapefile_path = \"C:/Users/jaden/Downloads/Research/Glaciers/RGI2000-v7.0-G-01_alaska/RGI2000-v7.0-G-01_alaska.shp\"\n",
    "\n",
    "xres,yres = (100.0,100.0)   # output (datacube) resolution in projection meters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371904bf-d6a5-4c6f-ba35-ca836dbcd911",
   "metadata": {},
   "source": [
    "## Select all glaciers by RGIId with sufficient coverage\n",
    "The following code is simply a function that is used to identify which glaciers are in a given scene and have coverage.  This then allows you to subset the individual glaciers once the scene is processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d545512-9ac7-411c-88fe-5358dfd772d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI data\n",
    "rgi_fp = \"C:/Users/jaden/Downloads/Research/Glaciers/\" # change folderpath\n",
    "rgi_cols_drop = ['glims_id', 'anlys_id', 'subm_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f41126ea-135f-4a4a-933f-5eb17a30036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectglaciersrgitable(glac_no=None, rgi_regionsO1=None, rgi_regionsO2='all', rgi_glac_number='all',\n",
    "                           rgi_fp=rgi_fp, rgi_cols_drop=rgi_cols_drop,\n",
    "                           include_landterm=True, include_laketerm=True, include_tidewater=True,\n",
    "                           glac_no_skip=None, min_glac_area_km2=0):\n",
    "    \"\"\"\n",
    "    Select all glaciers to be used in the model run according to the regions and glacier numbers defined by the RGI\n",
    "    glacier inventory. This function returns the rgi table associated with all of these glaciers.\n",
    "\n",
    "    glac_no : list of strings\n",
    "        list of strings of RGI glacier numbers (e.g., ['1.00001', '13.00001'])\n",
    "    rgi_regionsO1 : list of integers\n",
    "        list of integers of RGI order 1 regions (e.g., [1, 13])\n",
    "    rgi_regionsO2 : list of integers or 'all'\n",
    "        list of integers of RGI order 2 regions or simply 'all' for all the order 2 regions\n",
    "    rgi_glac_number : list of strings\n",
    "        list of RGI glacier numbers without the region (e.g., ['00001', '00002'])\n",
    "\n",
    "    Output: Pandas DataFrame of the glacier statistics for each glacier in the model run\n",
    "    (rows = GlacNo, columns = glacier statistics)\n",
    "    \"\"\"\n",
    "    if glac_no is not None:\n",
    "        glac_no_byregion = {}\n",
    "        rgi_regionsO1 = [int(i.split('.')[0]) for i in glac_no]\n",
    "        rgi_regionsO1 = list(set(rgi_regionsO1))\n",
    "        for region in rgi_regionsO1:\n",
    "            glac_no_byregion[region] = []\n",
    "        for i in glac_no:\n",
    "            region = i.split('.')[0]\n",
    "            glac_no_only = i.split('.')[1]\n",
    "            glac_no_byregion[int(region)].append(glac_no_only)\n",
    "\n",
    "        for region in rgi_regionsO1:\n",
    "            glac_no_byregion[region] = sorted(glac_no_byregion[region])\n",
    "\n",
    "    # Create an empty dataframe\n",
    "    rgi_regionsO1 = sorted(rgi_regionsO1)\n",
    "    glacier_table = pd.DataFrame()\n",
    "    for region in rgi_regionsO1:\n",
    "\n",
    "        if glac_no is not None:\n",
    "            rgi_glac_number = glac_no_byregion[region]\n",
    "\n",
    "        for i in os.listdir(rgi_fp):\n",
    "            if str(region).zfill(2) in i:\n",
    "                rgi_fp_reg = rgi_fp + i + '/'\n",
    "\n",
    "        for i in os.listdir(rgi_fp_reg):\n",
    "            if i.endswith('attributes.csv'):\n",
    "                rgi_fn = i\n",
    "                \n",
    "        try:\n",
    "            csv_regionO1 = pd.read_csv(rgi_fp_reg + rgi_fn)\n",
    "        except:\n",
    "            csv_regionO1 = pd.read_csv(rgi_fp_reg + rgi_fn, encoding='latin1')\n",
    "\n",
    "        # Populate glacer_table with the glaciers of interest\n",
    "        if rgi_regionsO2 == 'all' and rgi_glac_number == 'all':\n",
    "            print(\"All glaciers within region(s) %s are included in this model run.\" % (region))\n",
    "            if glacier_table.empty:\n",
    "                glacier_table = csv_regionO1\n",
    "            else:\n",
    "                glacier_table = pd.concat([glacier_table, csv_regionO1], axis=0)\n",
    "        elif rgi_regionsO2 != 'all' and rgi_glac_number == 'all':\n",
    "            print(\"All glaciers within subregion(s) %s in region %s are included in this model run.\" %\n",
    "                  (rgi_regionsO2, region))\n",
    "            for regionO2 in rgi_regionsO2:\n",
    "                regionO2_str = str(region).zfill(2) + '-' + str(regionO2).zfill(2)\n",
    "                if glacier_table.empty:\n",
    "                    glacier_table = csv_regionO1.loc[csv_regionO1['o2region'] == regionO2_str]\n",
    "                else:\n",
    "                    glacier_table = (pd.concat([glacier_table, csv_regionO1.loc[csv_regionO1['o2region'] ==\n",
    "                                                                                regionO2_str]], axis=0))\n",
    "        else:\n",
    "            if len(rgi_glac_number) < 20:\n",
    "                print(\"%s glaciers in region %s are included: %s\" % (len(rgi_glac_number), region, rgi_glac_number))\n",
    "            else:\n",
    "                print(\"%s glaciers in region %s are included\" % (len(rgi_glac_number), region))\n",
    "                \n",
    "            rgiid_subset = ['RGI2000-v7.0-G-' + str(region).zfill(2) + '-' + x.zfill(5) for x in rgi_glac_number]\n",
    "            rgiid_all = list(csv_regionO1.rgi_id.values)\n",
    "            rgi_idx = [rgiid_all.index(x) for x in rgiid_subset if x in rgiid_all]\n",
    "            if glacier_table.empty:\n",
    "                glacier_table = csv_regionO1.loc[rgi_idx]\n",
    "            else:\n",
    "                glacier_table = (pd.concat([glacier_table, csv_regionO1.loc[rgi_idx]],\n",
    "                                           axis=0))\n",
    "\n",
    "    glacier_table = glacier_table.copy()\n",
    "    # reset the index so that it is in sequential order (0, 1, 2, etc.)\n",
    "    glacier_table.reset_index(inplace=True)\n",
    "\n",
    "    # drop connectivity 2 for Greenland and Antarctica\n",
    "    glacier_table = glacier_table.loc[glacier_table['conn_lvl'].isin([0,1])]\n",
    "    glacier_table.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # add a simple glacier number column\n",
    "    glacier_table['glacno'] = [int(x.split('-')[-1]) for x in glacier_table.rgi_id.values]\n",
    "\n",
    "    # drop columns of data that is not being used\n",
    "    glacier_table.drop(rgi_cols_drop, axis=1, inplace=True)\n",
    "\n",
    "    # Longitude between 0-360deg (no negative)\n",
    "    glacier_table['cenlon_360'] = glacier_table['cenlon']\n",
    "    glacier_table.loc[glacier_table['cenlon'] < 0, 'cenlon_360'] = (\n",
    "            360 + glacier_table.loc[glacier_table['cenlon'] < 0, 'cenlon_360'])\n",
    "    # Subset glaciers based on their terminus type\n",
    "    termtype_values = []\n",
    "    if include_landterm:\n",
    "        termtype_values.append(0)\n",
    "        # assume dry calving, regenerated, and not assigned are land-terminating\n",
    "        termtype_values.append(3)\n",
    "        termtype_values.append(4)\n",
    "        termtype_values.append(9)\n",
    "    if include_tidewater:\n",
    "        termtype_values.append(1)\n",
    "        # assume shelf-terminating glaciers are tidewater\n",
    "        termtype_values.append(5)\n",
    "    if include_laketerm:\n",
    "        termtype_values.append(2)\n",
    "    glacier_table = glacier_table.loc[glacier_table['term_type'].isin(termtype_values)]\n",
    "    glacier_table.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Remove glaciers below threshold\n",
    "    glacier_table = glacier_table.loc[glacier_table['area_km2'] > min_glac_area_km2,:]\n",
    "    glacier_table.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Remove glaciers that are meant to be skipped\n",
    "    if glac_no_skip is not None:\n",
    "        glac_no_all = list(glacier_table['glacno'])\n",
    "        glac_no_unique = [x for x in glac_no_all if x not in glac_no_skip]\n",
    "        unique_idx = [glac_no_all.index(x) for x in glac_no_unique]\n",
    "        glacier_table = glacier_table.loc[unique_idx,:]\n",
    "        glacier_table.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # print(\"This study is focusing on %s glaciers in region %s\" % (glacier_table.shape[0], rgi_regionsO1))\n",
    "    return glacier_table\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8da390-552c-4775-9d19-7ea18b70c86a",
   "metadata": {},
   "source": [
    "# SAR Datacube Class\n",
    "This class is used to take an existing pre-processed datacube (processed using the code above) and have the data and functions all in one place for the estimation of glacier melt extents.  Some of the functions deal with quality control, while others deal with the actual implementation of the change pixel methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21111ae8-0d07-4b71-8d22-10feac78795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sar_datacube():\n",
    "    \"\"\"\n",
    "    SAR Datacube for melt extent analyses\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    ds_fn : str\n",
    "        filename of the datacube\n",
    "    scene_name : str\n",
    "        name of the scene for easier referencing and naming files\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 ds_fn=str(),\n",
    "                 scene_name=str(),\n",
    "                 rgi_reg=1,\n",
    "                 xres=None,\n",
    "                 yres=None,\n",
    "                 min_glac_area_km2=0,\n",
    "                 db_threshold=-3,\n",
    "                 db_threshold_sl=3,\n",
    "                 zscore_threshold=-2,\n",
    "                 winter_months=[1,2],\n",
    "                 snowmelt_months=[4,5,6,7],\n",
    "                 months2exclude_cp=[10, 11, 12, 1, 2],\n",
    "                 winter_std_threshold=3, # maximum winter standard deviation [dB]\n",
    "                 bin_size=20,\n",
    "                 area_bin_size=100000,\n",
    "                 allmelt_threshold=0.9,\n",
    "                 allmelt_pixels=10,\n",
    "                 nan_filter=-1e10, # value below which you can threshold for nan data\n",
    "                 min_area_frac=0.9, # minimum fraction of the total area that has data to be included\n",
    "                 subset_y = slice(500, 1000),\n",
    "                 subset_x = slice(500, 1000)\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Add attributes\n",
    "        \"\"\"\n",
    "        self.ds_fn = ds_fn\n",
    "        self.scene_name = scene_name\n",
    "        self.rgi_reg = rgi_reg\n",
    "\n",
    "        # Load xarray dataset\n",
    "        ds = xr.open_dataset(\n",
    "            self.ds_fn,\n",
    "            chunks={'x': 500, 'y': 500}   # all 226 timesteps kept in each chunk\n",
    "        )\n",
    "        self.ds = ds.isel(y=subset_y, x=subset_x)\n",
    "        \n",
    "        \n",
    "        self.data = (ds.images.isel(y=subset_y, x=subset_x)).values\n",
    "        if not nan_filter is None:\n",
    "            self.data[self.data < nan_filter] = np.nan\n",
    "        mask_good_pixels = np.sum(self.data, axis=0)\n",
    "        mask_good_pixels[~np.isnan(mask_good_pixels)] = 1\n",
    "        self.mask_good_pixels = mask_good_pixels\n",
    "        self.data_good = self.data * self.mask_good_pixels[np.newaxis,:,:]\n",
    "        \n",
    "        self.dates = self.ds.time.values\n",
    "\n",
    "        #self.mask_values = self.ds.rgi_ind_glacier_mask.values\n",
    "        self.mask_values = (ds.rgi_ind_glacier_mask.isel(y=subset_y, x=subset_x)).values\n",
    "        self.dem = self.ds.dem.values\n",
    "        self.dem = (ds.dem.isel(y=subset_y, x=subset_x)).values\n",
    "        self.xres = xres\n",
    "        self.yres = yres\n",
    "\n",
    "        # Single Glacier Dictionary initialization\n",
    "        self.glac_bounds = {}\n",
    "        self.glac_mask = {}\n",
    "        self.glac_mask_good_pixels = {}\n",
    "        self.glac_data = {}\n",
    "        self.glac_data_cp = {}\n",
    "        self.glac_data_sl_cp = {}\n",
    "        self.min_dB_elevs = {}\n",
    "        self.glac_dem = {}\n",
    "        self.glac_bins = {}\n",
    "        self.glac_bins_center = {}\n",
    "        self.glac_area_bins = {}\n",
    "        self.glac_area_bins_center = {}\n",
    "\n",
    "        self.glac_melt_extent_elevs_percentiles = {}\n",
    "        self.glac_melt_extent_elevs_percentile_mins = {}\n",
    "        self.glac_melt_extent_elevs_percentile_maxs = {}\n",
    "        self.glac_melt_extent_areas_percentiles = {}\n",
    "        self.glac_melt_extent_areas_percentile_mins = {}\n",
    "        self.glac_melt_extent_areas_percentile_maxs = {}\n",
    "\n",
    "        self.glac_snowline_elevs_percentiles = {}\n",
    "        self.glac_snowline_elevs_percentile_mins = {}\n",
    "        self.glac_snowline_elevs_percentile_maxs = {}\n",
    "        self.glac_snowline_areas_percentiles = {}\n",
    "        self.glac_snowline_areas_percentile_mins = {}\n",
    "        self.glac_snowline_areas_percentile_maxs = {}\n",
    "        \n",
    "\n",
    "        # Attributes\n",
    "        self.min_glac_area_km2 = min_glac_area_km2\n",
    "        self.db_threshold=db_threshold\n",
    "        self.db_threshold_sl=db_threshold_sl\n",
    "        self.zscore_threshold=zscore_threshold\n",
    "        self.winter_months=winter_months\n",
    "        self.snowmelt_months=snowmelt_months\n",
    "        self.months2exclude_cp=months2exclude_cp\n",
    "        self.winter_std_threshold=winter_std_threshold\n",
    "\n",
    "        self.bin_size = bin_size\n",
    "        self.area_bin_size = area_bin_size\n",
    "        self.allmelt_threshold = allmelt_threshold,\n",
    "        self.allmelt_pixels = allmelt_pixels\n",
    "        self.min_area_frac = min_area_frac\n",
    "    \n",
    "               \n",
    "    def glacnos_to_process(self):\n",
    "        \"\"\"\n",
    "        Identify glacier numbers to process\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        main_glac_rgi_sar : pd.DataFrame\n",
    "            dataframe of relevant RGI attributes and any added ones\n",
    "        \"\"\"\n",
    "        glacnos = sorted(list(np.unique(self.mask_values)))[1:]\n",
    "        glacnos_str = [str(rgi_reg) + '.' + str(x).zfill(5) for x in glacnos]\n",
    "        \n",
    "        assert len(glacnos_str) > 0, 'No glaciers to process'\n",
    "        main_glac_rgi_raw = selectglaciersrgitable(glac_no=glacnos_str, min_glac_area_km2=self.min_glac_area_km2)\n",
    "        glacnos_raw = list(main_glac_rgi_raw.glacno.values)\n",
    "        \n",
    "        # glacnos = glacnos_raw\n",
    "        \n",
    "        # Remove glaciers that are on the edge (and thus cut off and incomplete coverage)\n",
    "        self.mask_values = self.ds.rgi_ind_glacier_mask.values\n",
    "        glacno_edges = (list(np.unique(self.mask_values[0,:])) + \n",
    "                        list(np.unique(self.mask_values[-1,:])) + \n",
    "                        list(np.unique(self.mask_values[:,0])) + \n",
    "                        list(np.unique(self.mask_values[:,-1])))\n",
    "        glacno_edges = list(np.unique(np.array(glacno_edges)))\n",
    "        glacno_edges.remove(0)\n",
    "        glacnos = [x for x in glacnos_raw if x not in glacno_edges]\n",
    "        glac_idxs = [glacnos_raw.index(x) for x in glacnos]\n",
    "        main_glac_rgi = main_glac_rgi_raw.loc[glac_idxs]\n",
    "        main_glac_rgi.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        glacnos_2process = []\n",
    "        glacnos_dsfrac = []\n",
    "        glacnos_sarfrac = []\n",
    "        for nglac, glacno in enumerate(glacnos):\n",
    "            area_km2 = main_glac_rgi.loc[nglac,'area_km2']\n",
    "            area_ds = np.where(self.mask_values == glacno)[0].shape[0] * xres * yres / 1e6\n",
    "            area_ds_frac = area_ds / area_km2\n",
    "            \n",
    "            area_sar = np.where(~np.isnan(self.data_good[0,:,:][np.where(self.mask_values == glacno)]))[0].shape[0] * xres * yres / 1e6\n",
    "            area_sar_frac = area_sar / area_km2\n",
    "            if area_ds_frac > self.min_area_frac and area_sar_frac > self.min_area_frac:\n",
    "                glacnos_2process.append(glacno)\n",
    "                glacnos_dsfrac.append(area_ds_frac)\n",
    "                glacnos_sarfrac.append(area_sar_frac)\n",
    "        \n",
    "        assert len(glacnos_2process) > 0, 'No glaciers suitable for processing'\n",
    "        glacnos_2process_str = [str(rgi_reg) + '.' + str(x).zfill(5) for x in glacnos_2process]\n",
    "        main_glac_rgi_sar = selectglaciersrgitable(glac_no=glacnos_2process_str)\n",
    "        main_glac_rgi_sar['ds_area_frac'] = glacnos_dsfrac\n",
    "        # main_glac_rgi_sar['sar_area_frac'] = glacnos_sarfrac\n",
    "        main_glac_rgi_sar['rgino_str'] = [str(main_glac_rgi_sar.loc[x,'o1region']).zfill(2) + '.' + \n",
    "                                          str(main_glac_rgi_sar.loc[x,'glacno']).zfill(5) for x in np.arange(main_glac_rgi_sar.shape[0])]\n",
    "        \n",
    "        return main_glac_rgi_sar\n",
    "        \n",
    "\n",
    "    def mask_nonglacier_pixels(self, main_glac_rgi):\n",
    "        \"\"\"\n",
    "        Mask the non-glaciated pixels\n",
    "        \"\"\"\n",
    "        self.mask_values = self.ds.rgi_ind_glacier_mask.values\n",
    "\n",
    "        mask_values_minsize = np.zeros(self.mask_values.shape)\n",
    "        for glacno in main_glac_rgi.glacno.values:\n",
    "            mask_values_minsize[self.mask_values == glacno] = glacno\n",
    "        mask_values_minsize_binary = np.zeros(self.mask_values.shape)\n",
    "        mask_values_minsize_binary[mask_values_minsize>0] = 1\n",
    "        \n",
    "        data_masked = np.copy(self.data_good)\n",
    "        data_masked[self.data_good < -1e10] = np.nan # filtering out very large negative values\n",
    "        for nscene in np.arange(self.data_good.shape[0]):\n",
    "            data_scene = data_masked[nscene,:,:]\n",
    "            data_scene[mask_values_minsize==0] = np.nan\n",
    "            data_masked[nscene,:,:] = data_scene\n",
    "        \n",
    "        self.data_masked = data_masked\n",
    "          \n",
    "    def pixel_analysis(self):\n",
    "        dates_pd = pd.DatetimeIndex(self.dates)\n",
    "        self.years = [x.year for x in dates_pd]\n",
    "        self.months = [x.month for x in dates_pd]\n",
    "        self.days = [x.day for x in dates_pd]\n",
    "        self.doys = [int(x.to_julian_date() - pd.Timestamp(x.year,1,1).to_julian_date()) for x in dates_pd]\n",
    "        \n",
    "        winter_idx = [idx for idx, element in enumerate(self.months) if element in self.winter_months]\n",
    "        \n",
    "        data_winter_mean = np.nanmean(self.data_masked[winter_idx,:,:], axis=0)\n",
    "        data_winter_std = np.nanstd(self.data_masked[winter_idx,:,:], axis=0)\n",
    "        data_winter_res = self.data_masked - data_winter_mean[np.newaxis,:,:]\n",
    "        self.data_zscore = data_winter_res / data_winter_std[np.newaxis,:,:]\n",
    "\n",
    "        data_cp = np.zeros(self.data_masked.shape)\n",
    "        data_cp[np.isnan(self.data_masked)] = np.nan\n",
    "        data_cp[(data_winter_res < self.db_threshold) & (self.data_zscore < self.zscore_threshold)] = 1\n",
    "        self.data_cp = data_cp\n",
    "\n",
    "        # snowline change pixels\n",
    "        data_sl_cp = np.zeros(self.data_masked.shape)\n",
    "        data_sl_cp[np.isnan(self.data_masked)] = np.nan\n",
    "\n",
    "        # snowline change pixels -- based on minimum backscatter from each year\n",
    "        summer_idx = [idx for idx, element in enumerate(self.months) if element in self.snowmelt_months]\n",
    "        for yr in set(self.years):\n",
    "            year_idx = [idx for idx, y in enumerate(self.years) if y == yr]\n",
    "            comb_idx = list(set(year_idx).intersection(summer_idx))\n",
    "            if comb_idx: # ensure that we have any data for the year\n",
    "                # data_summer_min_yr = np.nanpercentile(self.data_masked[comb_idx,:,:], 5, axis=0) # get dB for the 5% of melt pixels\n",
    "                data_summer_min_yr = nan_percentile(self.data_masked[comb_idx,:,:], q=5, axis=0) # faster nanpercentile alternative\n",
    "                data_summer_res_yr = self.data_masked[year_idx,:,:] - data_summer_min_yr[np.newaxis,:,:]\n",
    "\n",
    "                # mask for the indices of the given year\n",
    "                for i, idx in enumerate(year_idx):\n",
    "                    mask = (data_winter_res[idx] > self.db_threshold) | (data_summer_res_yr[i] > self.db_threshold_sl)\n",
    "                    data_sl_cp[idx][mask] = 1\n",
    "        \n",
    "        # data_summer_min = np.nanpercentile(self.data_masked[summer_idx,:,:], 5, axis=0) # get dB for the 5% of melt pixels\n",
    "        # data_summer_res = self.data_masked[summer_idx,:,:] - data_summer_min[np.newaxis,:,:]\n",
    "        # data_sl_cp[(data_winter_res > self.db_threshold) | (data_summer_res > self.db_threshold_sl)] = 1\n",
    "        self.data_sl_cp = data_sl_cp\n",
    "\n",
    "        \n",
    "\n",
    "    def diagnose_snowline_activity(self):\n",
    "        \"\"\"Quick diagnostic plot of mean snowline activity per month.\"\"\"\n",
    "        dates_pd = pd.DatetimeIndex(self.dates)\n",
    "        months = np.array([d.month for d in dates_pd])\n",
    "\n",
    "        # mean fraction of valid pixels flagged as 1 for each timestep\n",
    "        frac_active = np.nanmean(self.data_sl_cp, axis=(1, 2))\n",
    "\n",
    "        # monthly average\n",
    "        df = pd.DataFrame({'month': months, 'frac': frac_active})\n",
    "        monthly = df.groupby('month')['frac'].mean()\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(monthly.index, monthly.values, marker='o', lw=2)\n",
    "        plt.title(\"Mean Snowline Change Activity by Month\")\n",
    "        plt.xlabel(\"Month\")\n",
    "        plt.ylabel(\"Fraction of pixels flagged (mean)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(monthly)\n",
    "\n",
    "    def annual_melt_onset_map(self):\n",
    "\n",
    "        \n",
    "        self.melt_onset_doy_maps = {}\n",
    "        years_unique = np.unique(self.years)\n",
    "        for nyear, year in enumerate(years_unique):\n",
    "            # Subset dates for the given year\n",
    "            year_idx = list(np.where(np.array(self.years) == year)[0])\n",
    "            months_subset = [self.months[x] for x in year_idx]\n",
    "            doys_subset = [self.doys[x] for x in year_idx]\n",
    "        \n",
    "            # Prevent melt/SL onset in winter months\n",
    "            data_cp_year = self.data_cp[year_idx,:,:]\n",
    "            \n",
    "            for nmonth, month in enumerate(months_subset):\n",
    "                if month in self.months2exclude_cp:\n",
    "                    data_cp_year[nmonth,:,:] = 0\n",
    "\n",
    "            # Get the first value of 1 (i.e., first change pixel)\n",
    "            data_cp_year_onset_idx = (data_cp_year != 0).argmax(axis=0)\n",
    "            data_cp_year_onset_idx = data_cp_year_onset_idx * self.mask_good_pixels\n",
    "        \n",
    "            # Only index months where there's a value of 1\n",
    "            data_cp_year_sum = data_cp_year.sum(0)\n",
    "            data_cp_year_sum[data_cp_year_sum>0] = 1\n",
    "        \n",
    "            # Remove pixels where there is no melt\n",
    "            data_cp_year_sum[np.isnan(data_cp_year_sum)] = 0\n",
    "            data_cp_year_onset_idx[data_cp_year_sum == 0] = np.nan\n",
    "        \n",
    "            # Plot the julian day of melt onset\n",
    "            onset_idx_unique = np.unique(data_cp_year_onset_idx)\n",
    "            data_cp_year_onset_doy = np.zeros(data_cp_year_onset_idx.shape)\n",
    "\n",
    "            for onset_idx in onset_idx_unique:\n",
    "                if not np.isnan(onset_idx):\n",
    "                    onset_idx = int(onset_idx)\n",
    "                    doy = doys_subset[onset_idx]\n",
    "                    data_cp_year_onset_doy[data_cp_year_onset_idx == onset_idx] = doy            \n",
    "        \n",
    "            data_cp_year_onset_doy[data_cp_year_onset_doy==0] = np.nan\n",
    "\n",
    "            self.melt_onset_doy_maps[year] = data_cp_year_onset_doy\n",
    "\n",
    "    def generate_snowline_post_onset_mask(self):\n",
    "        \"\"\"\n",
    "        Generate a mask of shape (T, H, W) where each pixel is 1 if:\n",
    "        - The current DOY is >= the melt onset DOY for that pixel (per year)\n",
    "        - AND the summer residual (backscatter - summer_min) exceeds the \n",
    "            snowline threshold self.db_threshold_sl.\n",
    "\n",
    "        Output stored as: self.data_sl_post_onset\n",
    "        \"\"\"\n",
    "\n",
    "        # Allocate output mask\n",
    "        T, H, W = self.data_masked.shape\n",
    "        data_sl_post_onset = np.zeros((T, H, W), dtype=float)\n",
    "        data_sl_post_onset[:] = 0   # default zero\n",
    "\n",
    "        # Precompute residuals for entire cube ------------------------------\n",
    "        summer_idx = [i for i, m in enumerate(self.months) if m in self.snowmelt_months]\n",
    "\n",
    "        # For each year, compute summer-min and residuals\n",
    "        years_unique = np.unique(self.years)\n",
    "\n",
    "        for year in years_unique:\n",
    "\n",
    "            # Indices for this year\n",
    "            year_idx = np.where(np.array(self.years) == year)[0]\n",
    "            months_subset = [self.months[i] for i in year_idx]\n",
    "\n",
    "            # Get melt onset DOY map (2D)\n",
    "            onset_map = self.melt_onset_doy_maps[year]   # shape (H, W)\n",
    "\n",
    "            # Compute summer-min for this year (same logic as pixel_analysis)\n",
    "            summer_idx_year = list(set(year_idx).intersection(summer_idx))\n",
    "            if len(summer_idx_year) == 0:\n",
    "                continue\n",
    "\n",
    "            data_summer_min_yr = nan_percentile(\n",
    "                self.data_masked[summer_idx_year, :, :],\n",
    "                q=5, axis=0\n",
    "            )\n",
    "\n",
    "            # Compute summer residual for this year\n",
    "            data_summer_res_yr = (\n",
    "                self.data_masked[year_idx, :, :] - \n",
    "                data_summer_min_yr[np.newaxis, :, :]\n",
    "            )  # shape: (#year_frames, H, W)\n",
    "\n",
    "\n",
    "            # Loop through each timestep of the year -------------------------\n",
    "            for k, t in enumerate(year_idx):\n",
    "                \n",
    "                month_t = self.months[t]\n",
    "                # DOY of this timestep\n",
    "                doy_t = self.doys[t]\n",
    "\n",
    "                if month_t in self.months2exclude_cp:\n",
    "                    # Set everything to 0\n",
    "                    tmp = np.zeros((H, W), dtype=float)\n",
    "                    tmp[self.mask_good_pixels == 1] = 0\n",
    "                    data_sl_post_onset[t] = tmp\n",
    "                    continue\n",
    "\n",
    "                # Condition 1: DOY must be > local melt-onset DOY\n",
    "                cond_doy = doy_t > onset_map\n",
    "\n",
    "                # Condition 2: residual > SL threshold\n",
    "                cond_res = data_summer_res_yr[k] > self.db_threshold_sl\n",
    "\n",
    "                # Combine conditions\n",
    "                mask = cond_doy & cond_res & (self.mask_good_pixels == 1)\n",
    "\n",
    "                \n",
    "                data_sl_post_onset[t][:] = 0\n",
    "                data_sl_post_onset[t][mask] = 1\n",
    "\n",
    "        # Save result\n",
    "        self.data_sl_post_onset = data_sl_post_onset\n",
    "\n",
    "        # year_debug = 2018\n",
    "        # onset_map_2018 = self.melt_onset_doy_maps[year_debug]\n",
    "        # print(\"2018 onset DOY min/max:\",\n",
    "        #     np.nanmin(onset_map_2018[self.mask_good_pixels == 1]),\n",
    "        #     np.nanmax(onset_map_2018[self.mask_good_pixels == 1]))\n",
    "\n",
    "        # # First index for 2018:\n",
    "        # first_2018_idx = np.where(np.array(self.years) == year_debug)[0][0]\n",
    "        # print(\"First 2018 frame DOY:\", self.doys[first_2018_idx])\n",
    "\n",
    "\n",
    "        # Optional plot for debugging\n",
    "        # plt.figure(figsize=(7, 5))\n",
    "        # plt.imshow(np.nanmean(data_sl_post_onset, axis=0), cmap=\"gray\")\n",
    "        # plt.title(\"Mean Snowline-Post-Onset Mask\")\n",
    "        # plt.colorbar()\n",
    "        # plt.show()\n",
    "\n",
    "        # for i in range(min(40, len(self.dates))):\n",
    "\n",
    "        #     date_str = np.datetime_as_string(self.dates[i], unit='D')\n",
    "        #     doy = self.doys[i]\n",
    "        #     year = self.years[i]\n",
    "\n",
    "        #     frame = self.data_sl_post_onset[i]\n",
    "\n",
    "        #     white_pixels = np.argwhere(frame == 1)\n",
    "        #     nan_pixels   = np.argwhere(np.isnan(frame))\n",
    "\n",
    "        #     print(\"num white pixels:\", len(white_pixels))\n",
    "        #     print(\"num nan pixels:\", len(nan_pixels))\n",
    "\n",
    "        #     # Melt onset DOY map for that year (2D array)\n",
    "        #     melt_onset_map = self.melt_onset_doy_maps[year]\n",
    "\n",
    "        #     fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        #     # --- Panel 1: Snowline post-onset mask ---\n",
    "        #     ax[0].imshow(self.data_sl_post_onset[i], cmap=\"gray\", vmin=0, vmax=1)\n",
    "        #     ax[0].set_title(f\"SL Post-Onset — {date_str}\\n(DOY {doy})\")\n",
    "        #     ax[0].axis(\"off\")\n",
    "\n",
    "        #     # --- Panel 2: Melt onset DOY map ---\n",
    "        #     im = ax[1].imshow(melt_onset_map, cmap=\"plasma\")\n",
    "        #     ax[1].set_title(f\"Melt Onset DOY Map — {year}\")\n",
    "        #     ax[1].axis(\"off\")\n",
    "\n",
    "        #     # Add colorbar for the melt onset map\n",
    "        #     fig.colorbar(im, ax=ax[1], shrink=0.75, label=\"Day of Year\")\n",
    "\n",
    "        #     plt.tight_layout()\n",
    "        #     plt.show()\n",
    "\n",
    "    def annual_snowline_post_onset_map(self):\n",
    "        \"\"\"\n",
    "        Generate the first-true (DOY) map for snowline-post-onset masks,\n",
    "        analogous to annual_melt_onset_map(), but using self.data_sl_post_onset.\n",
    "\n",
    "        Output stored as:\n",
    "            self.snowline_post_onset_doy_maps[year] = 2D DOY map\n",
    "        \"\"\"\n",
    "\n",
    "        self.snowline_post_onset_doy_maps = {}\n",
    "\n",
    "        years_unique = np.unique(self.years)\n",
    "\n",
    "        for year in years_unique:\n",
    "\n",
    "            # Get indices for this year\n",
    "            year_idx = list(np.where(np.array(self.years) == year)[0])\n",
    "            months_subset = [self.months[i] for i in year_idx]\n",
    "            doys_subset   = [self.doys[i]   for i in year_idx]\n",
    "\n",
    "            # Extract SL-post-onset mask for this year\n",
    "            data_sl_year = self.data_sl_post_onset[year_idx, :, :]  # shape (#frames, H, W)\n",
    "\n",
    "            # --- Apply winter exclusion ---------------------------------------\n",
    "            for nmonth, month in enumerate(months_subset):\n",
    "                if month in self.months2exclude_cp:\n",
    "                    data_sl_year[nmonth, :, :] = 0   # no SL-onset in winter\n",
    "\n",
    "            # --- First detection index per-pixel -------------------------------\n",
    "            data_sl_onset_idx = (data_sl_year != 0).argmax(axis=0) * self.mask_good_pixels\n",
    "\n",
    "            # --- Remove pixels with no snowline change -------------------------\n",
    "            data_sl_sum = np.nansum(data_sl_year, axis=0)\n",
    "            data_sl_sum[data_sl_sum > 0] = 1\n",
    "\n",
    "            data_sl_sum[np.isnan(data_sl_sum)] = 0\n",
    "            data_sl_onset_idx[data_sl_sum == 0] = np.nan\n",
    "\n",
    "            # --- Convert onset index → DOY map ---------------------------------\n",
    "            data_sl_onset_doy = np.zeros_like(data_sl_onset_idx, dtype=float)\n",
    "\n",
    "            unique_onsets = np.unique(data_sl_onset_idx)\n",
    "\n",
    "            for onset_idx in unique_onsets:\n",
    "                if not np.isnan(onset_idx):\n",
    "                    onset_idx = int(onset_idx)\n",
    "                    doy = doys_subset[onset_idx]\n",
    "                    data_sl_onset_doy[data_sl_onset_idx == onset_idx] = doy\n",
    "\n",
    "            data_sl_onset_doy[data_sl_onset_doy == 0] = np.nan\n",
    "\n",
    "            # Save\n",
    "            self.snowline_post_onset_doy_maps[year] = data_sl_onset_doy\n",
    "\n",
    "            # ---------------------------------------------------------------\n",
    "            # Diagnostic plot\n",
    "            # ---------------------------------------------------------------\n",
    "            # plt.figure(figsize=(7, 6))\n",
    "            # im = plt.imshow(data_sl_onset_doy, cmap=\"plasma\")\n",
    "            # plt.title(f\"Snowline Post-Onset DOY — {year}\")\n",
    "            # plt.colorbar(im, label=\"Day of Year\")\n",
    "            # plt.tight_layout()\n",
    "            # plt.show()\n",
    "    \n",
    "    def plot_snowline_vs_melt_onsets(self, cmap=\"bwr\", vmin=-20, vmax=60):\n",
    "        \"\"\"\n",
    "        Plot pixel-wise DOY difference between *post-onset* snowline and melt onset:\n",
    "            ΔDOY = SL_post_onset_DOY - melt_onset_DOY\n",
    "\n",
    "        Also prints how many pixels have SL < melt (which should be 0 if\n",
    "        snowline_post_onset was constructed with DOY >= melt_onset enforced).\n",
    "        \"\"\"\n",
    "\n",
    "        # Make sure the right maps exist\n",
    "        if not hasattr(self, \"snowline_post_onset_doy_maps\"):\n",
    "            raise ValueError(\n",
    "                \"snowline_post_onset_doy_maps not found. \"\n",
    "                \"Run annual_snowline_post_onset_map() (the new one) first.\"\n",
    "            )\n",
    "        if not hasattr(self, \"melt_onset_doy_maps\"):\n",
    "            raise ValueError(\n",
    "                \"melt_onset_doy_maps not found. \"\n",
    "                \"Run annual_melt_onset_map() first.\"\n",
    "            )\n",
    "\n",
    "        years = sorted(self.melt_onset_doy_maps.keys())\n",
    "\n",
    "        for year in years:\n",
    "            if year not in self.snowline_post_onset_doy_maps:\n",
    "                print(f\"[{year}] No snowline_post_onset_doy_map — skipping.\")\n",
    "                continue\n",
    "\n",
    "            melt_map = np.array(self.melt_onset_doy_maps[year], dtype=float)\n",
    "            sl_map   = np.array(self.snowline_post_onset_doy_maps[year], dtype=float)\n",
    "\n",
    "            # Mask off-glacier\n",
    "            melt_map[self.mask_good_pixels == 0] = np.nan\n",
    "            sl_map[self.mask_good_pixels == 0]   = np.nan\n",
    "\n",
    "            # Difference\n",
    "            diff = sl_map - melt_map   # shape (H, W)\n",
    "\n",
    "            # Where both are valid\n",
    "            valid = (~np.isnan(melt_map)) & (~np.isnan(sl_map))\n",
    "\n",
    "            num_valid = np.count_nonzero(valid)\n",
    "            if num_valid == 0:\n",
    "                print(f\"[{year}] No valid pixels for comparison.\")\n",
    "                continue\n",
    "\n",
    "            # --- Consistency check: SL should not be earlier than melt if post-onset is enforced ---\n",
    "            neg_mask = valid & (diff < 0)\n",
    "            num_neg  = np.count_nonzero(neg_mask)\n",
    "            num_pos  = np.count_nonzero(valid & (diff > 0))\n",
    "            num_zero = np.count_nonzero(valid & (diff == 0))\n",
    "\n",
    "            print(f\"\\nYear {year}:\")\n",
    "            print(f\"  Valid pixels:           {num_valid}\")\n",
    "            print(f\"  ΔDOY > 0 (SL after melt): {num_pos}\")\n",
    "            print(f\"  ΔDOY = 0 (same day):      {num_zero}\")\n",
    "            print(f\"  ΔDOY < 0 (SL BEFORE melt): {num_neg}\")\n",
    "\n",
    "            if num_neg > 0:\n",
    "                # Optional: mark these as NaN so they don't dominate the plot\n",
    "                # (or you can keep them to inspect)\n",
    "                diff[neg_mask] = np.nan\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(7, 6))\n",
    "            im = plt.imshow(diff, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "            plt.title(f\"Snowline Post-Onset DOY − Melt Onset DOY (Year {year})\")\n",
    "            plt.colorbar(im, label=\"Δ DOY (SL_post_onset − Melt)\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def print_snowline_melt_difference_diagnostics(self):\n",
    "        \"\"\"\n",
    "        Print diagnostic summaries showing the distribution of DOY differences:\n",
    "\n",
    "            ΔDOY = snowline_post_onset_DOY - melt_onset_DOY\n",
    "\n",
    "        Grouped into ranges:\n",
    "            Δ=0, 1–5, 6–10, 11–20, >20\n",
    "        \"\"\"\n",
    "\n",
    "        years = sorted(self.melt_onset_doy_maps.keys())\n",
    "\n",
    "        for year in years:\n",
    "\n",
    "            if year not in self.snowline_post_onset_doy_maps:\n",
    "                print(f\"[{year}] No snowline_post_onset_doy_map — skipping.\")\n",
    "                continue\n",
    "\n",
    "            melt_map = np.array(self.melt_onset_doy_maps[year], dtype=float)\n",
    "            sl_map   = np.array(self.snowline_post_onset_doy_maps[year], dtype=float)\n",
    "\n",
    "            # Mask off-glacier\n",
    "            melt_map[self.mask_good_pixels == 0] = np.nan\n",
    "            sl_map[self.mask_good_pixels == 0]   = np.nan\n",
    "\n",
    "            # Difference\n",
    "            diff = sl_map - melt_map\n",
    "\n",
    "            # Valid pixels\n",
    "            valid = (~np.isnan(diff))\n",
    "            diffs = diff[valid]\n",
    "\n",
    "            if diffs.size == 0:\n",
    "                print(f\"[{year}] No valid pixels.\")\n",
    "                continue\n",
    "\n",
    "            # Group counts\n",
    "            zero     = np.sum(diffs == 0)\n",
    "            r1_5     = np.sum((diffs >= 1)  & (diffs <= 5))\n",
    "            r6_10    = np.sum((diffs >= 6)  & (diffs <= 10))\n",
    "            r11_20   = np.sum((diffs >= 11) & (diffs <= 20))\n",
    "            r21_40   = np.sum((diffs >= 21) & (diffs <= 40))\n",
    "            r41plus  = np.sum(diffs > 40)\n",
    "\n",
    "            # Print nicely\n",
    "            print(f\"\\n===== YEAR {year} =====\")\n",
    "            print(f\"Total valid pixels: {len(diffs)}\")\n",
    "            print(f\"ΔDOY = 0:           {zero}\")\n",
    "            print(f\"ΔDOY 1–5:           {r1_5}\")\n",
    "            print(f\"ΔDOY 6–10:          {r6_10}\")\n",
    "            print(f\"ΔDOY 11–20:         {r11_20}\")\n",
    "            print(f\"ΔDOY 21–40:         {r21_40}\")\n",
    "            print(f\"ΔDOY >40:           {r41plus}\")\n",
    "\n",
    "            # extra: mean/median for quick sanity check\n",
    "            print(f\"Mean ΔDOY:          {np.round(np.mean(diffs), 2)}\")\n",
    "            print(f\"Median ΔDOY:        {np.round(np.median(diffs), 2)}\")\n",
    "\n",
    "    def save_snowmelt_onset_tiffs(self, out_dir):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        # get geoinfo from the xarray dataset\n",
    "        gt = self.ds.attrs.get('geotransform', None)\n",
    "        crs = self.ds.attrs.get('projection', None)\n",
    "\n",
    "        if gt is None:\n",
    "            raise ValueError(\"Dataset is missing 'geotransform' in attrs.\")\n",
    "        if crs is None:\n",
    "            raise ValueError(\"Dataset is missing 'projection' in attrs.\")\n",
    "\n",
    "        # turn GDAL-style geotransform into rasterio Affine\n",
    "        # geotransform = [x_min, x_res, 0, y_max, 0, y_res_neg]\n",
    "        transform = Affine.from_gdal(*gt)\n",
    "\n",
    "        for year, onset_arr in self.snowline_post_onset_doy_maps.items():\n",
    "            # make sure it's float32 so NaNs work\n",
    "            arr = onset_arr.astype(\"float32\")\n",
    "\n",
    "            height, width = arr.shape\n",
    "\n",
    "            out_fn = os.path.join(out_dir, f\"snowline_onset_{year}.tif\")\n",
    "\n",
    "            with rasterio.open(\n",
    "                out_fn,\n",
    "                \"w\",\n",
    "                driver=\"GTiff\",\n",
    "                height=height,\n",
    "                width=width,\n",
    "                count=1,\n",
    "                dtype=\"float32\",\n",
    "                crs=crs,\n",
    "                transform=transform,\n",
    "                nodata=np.nan,\n",
    "            ) as dst:\n",
    "                dst.write(arr, 1)\n",
    "            # sanity check plot\n",
    "            # plt.figure(figsize=(6, 5))\n",
    "            # im = plt.imshow(arr, cmap=\"viridis\")\n",
    "            # plt.title(f\"Melt onset DOY {year}\")\n",
    "            # plt.colorbar(im, label=\"Day of Year\")\n",
    "            # plt.tight_layout()\n",
    "            # plt.show()\n",
    "\n",
    "    def save_melt_onset_tiffs(self, out_dir):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        # get geoinfo from the xarray dataset\n",
    "        gt = self.ds.attrs.get('geotransform', None)\n",
    "        crs = self.ds.attrs.get('projection', None)\n",
    "\n",
    "        if gt is None:\n",
    "            raise ValueError(\"Dataset is missing 'geotransform' in attrs.\")\n",
    "        if crs is None:\n",
    "            raise ValueError(\"Dataset is missing 'projection' in attrs.\")\n",
    "\n",
    "        # turn GDAL-style geotransform into rasterio Affine\n",
    "        # geotransform = [x_min, x_res, 0, y_max, 0, y_res_neg]\n",
    "        transform = Affine.from_gdal(*gt)\n",
    "\n",
    "        for year, onset_arr in self.melt_onset_doy_maps.items():\n",
    "            # make sure it's float32 so NaNs work\n",
    "            arr = onset_arr.astype(\"float32\")\n",
    "\n",
    "            height, width = arr.shape\n",
    "\n",
    "            out_fn = os.path.join(out_dir, f\"melt_onset_{year}.tif\")\n",
    "\n",
    "            with rasterio.open(\n",
    "                out_fn,\n",
    "                \"w\",\n",
    "                driver=\"GTiff\",\n",
    "                height=height,\n",
    "                width=width,\n",
    "                count=1,\n",
    "                dtype=\"float32\",\n",
    "                crs=crs,\n",
    "                transform=transform,\n",
    "                nodata=np.nan,\n",
    "            ) as dst:\n",
    "                dst.write(arr, 1)\n",
    "            # quick sanity check plot\n",
    "            # plt.figure(figsize=(6, 5))\n",
    "            # im = plt.imshow(arr, cmap=\"viridis\")\n",
    "            # plt.title(f\"Melt onset DOY {year}\")\n",
    "            # plt.colorbar(im, label=\"Day of Year\")\n",
    "            # plt.tight_layout()\n",
    "            # plt.show()\n",
    "\n",
    "    def generate_melt_extent_elevations_from_onset(self, glacno, out_dir, \n",
    "                                                doy_step=10,\n",
    "                                                percentile=1.0,\n",
    "                                                min_valid_frac=0.01,\n",
    "                                                plot_year=2024,\n",
    "                                                plot_dir=None):\n",
    "                                                \n",
    "        plot_dir = os.path.join(os.getcwd(), 'testing')\n",
    "        # print(\"test::\")\n",
    "        # print((self.glac_data[6047]).shape)\n",
    "        \n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        mask_glac = self.glac_mask[glacno] \n",
    "        total_pixels = np.nansum(mask_glac)\n",
    "\n",
    "        for year, onset_map in self.melt_onset_doy_maps.items():\n",
    "            #print(f\"Processing year {year}...\")\n",
    "\n",
    "            # Mask valid glacier pixels\n",
    "\n",
    "                 \n",
    "            h, w = mask_glac.shape\n",
    "\n",
    "            #print(h, w) #19, 38\n",
    "\n",
    "            xb0 = self.glac_bounds[glacno]['xmin']\n",
    "            yb0 = self.glac_bounds[glacno]['ymin']\n",
    "\n",
    "            # Slice onset_map using height/width from mask\n",
    "            onset_glac = onset_map[xb0 : xb0 + h,      # h rows\n",
    "                                  yb0 : yb0 + w]       # w cols\n",
    "            # print(\"Bounds:\", yb0, yb0 + w, xb0, xb0 + h)\n",
    "            # # 373 411 55 74\n",
    "\n",
    "            # plt.figure(figsize=(6,6))\n",
    "            # plt.imshow(onset_map, cmap=\"viridis\")\n",
    "            # plt.scatter([373, 411], [55, 74],  c='red')   # bounding box corners\n",
    "            # plt.title(\"Full onset_map with glacier bbox markers\")\n",
    "            # plt.show()\n",
    "\n",
    "            # --- 1. Glacier Mask ---\n",
    "            # plt.imshow(mask_glac, cmap=\"gray\")\n",
    "            # #plt.set_title(f\"mask_glac {glacno}\" if glacno else \"mask_glac\")\n",
    "            # plt.axis(\"off\")\n",
    "            \n",
    "            # Now these shapes match: (27, 28) vs (27, 28)\n",
    "            onset_clipped = np.where(mask_glac == 1, onset_glac, np.nan)\n",
    "\n",
    "            onset_map = onset_clipped\n",
    "            \n",
    "           \n",
    "\n",
    "            # print(onset_glac.shape)\n",
    "            # print(mask_glac.shape)\n",
    "\n",
    "            valid_mask = ((self.glac_mask[glacno]) == 1) & (~np.isnan(onset_map))\n",
    "            onset_flat = onset_map[valid_mask]\n",
    "            dem_flat = (self.glac_dem[glacno])[valid_mask]\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            # print(f\"\\n=== Year {year} ===\")\n",
    "            # print(\"Total pixels:\", total_pixels)\n",
    "            # print(\"mask_good_pixels == 1:\", np.sum(mask_glac == 1))\n",
    "            # print(\"onset_map NOT nan:\", np.sum(~np.isnan(onset_map)))\n",
    "            # print(\"valid_mask True:\", np.sum(valid_mask))\n",
    "            # print(\"onset_flat.size:\", onset_flat.size)\n",
    "            # print(\"Fraction valid:\", onset_flat.size / total_pixels)\n",
    "\n",
    "            if onset_flat.size / total_pixels < min_valid_frac:\n",
    "                print(f\"  Skipping {year}: insufficient valid pixels.\")\n",
    "                continue\n",
    "\n",
    "            min_doy = int(np.nanmin(onset_flat))\n",
    "            max_doy = int(np.nanmax(onset_flat))\n",
    "            doys = np.arange(1, 366, doy_step)\n",
    "\n",
    "            results = []\n",
    "\n",
    "            # Derive pixel area from DEM \n",
    "            pixel_area_m2 = self.xres * self.yres\n",
    "            total_glacier_area_m2 = np.sum(self.glac_mask[glacno] == 1) * pixel_area_m2\n",
    "\n",
    "            for doy in doys:\n",
    "                                # build 2D melted mask for this DOY\n",
    "                melted_2d = (onset_map <= doy)\n",
    "\n",
    "                # require at least 3 melted neighbors\n",
    "                \n",
    "                kernel = np.ones((3,3), int)\n",
    "                neighbor_count = convolve(melted_2d.astype(int), kernel, mode=\"constant\", cval=0)\n",
    "                melted_2d_clean = melted_2d & (neighbor_count >= 3)\n",
    "\n",
    "                # now flatten using valid_mask\n",
    "                melt_elevs = np.sort((self.glac_dem[glacno])[ (self.glac_mask[glacno] == 1) & melted_2d_clean ])\n",
    "\n",
    "                melted = onset_flat <= doy\n",
    "                n_melt = np.sum(melted)\n",
    "\n",
    "\n",
    "                if melt_elevs.size == 0:\n",
    "                    melt_extent_elev = np.nan\n",
    "                    melt_area_m2 = 0.0\n",
    "                else:\n",
    "                    \n",
    "                    #melt_elevs = np.sort(dem_flat[melted])\n",
    "                    idx = int(percentile * (len(melt_elevs) - 1))\n",
    "                    melt_extent_elev = melt_elevs[idx]\n",
    "                    melt_area_m2 = melt_elevs.size * pixel_area_m2\n",
    "\n",
    "                # Convert DOY → datetime\n",
    "                date_obj = date(year, 1, 1) + timedelta(days=int(doy) - 1)\n",
    "                results.append({\n",
    "                    \"year\": year,\n",
    "                    \"date\": date_obj.strftime(\"%Y-%m-%d\"),\n",
    "                    \"melt_extent_elev_m\": melt_extent_elev,\n",
    "                    \"melt_area_m2\": melt_area_m2,\n",
    "                    \"melt_fraction\": melt_area_m2 / total_glacier_area_m2  # normalized fraction of glacier melted\n",
    "                })\n",
    "\n",
    "                 # --- Plot melt mask for specified year ---\n",
    "                plot_dir = None #not plotting rn\n",
    "                if plot_dir and year == plot_year:\n",
    "                    #glacno=6047\n",
    "                    #print(f\"Plotting DOY {doy} for {year}...\")\n",
    "\n",
    "                    # Convert DOY → datetime\n",
    "                    date_obj = date(year, 1, 1) + timedelta(days=int(doy) - 1)\n",
    "                    doy_target = np.datetime64(date_obj)\n",
    "\n",
    "                    # Get global SAR date list and cube for this glacier\n",
    "                    glac_cube = self.data_masked        # shape (time, y, x)\n",
    "                    #print(\"data masked shape:\", glac_cube.shape)\n",
    "                    glac_times = np.array(self.dates).astype(\"datetime64[D]\")\n",
    "\n",
    "                    # Find the closest acquisition date\n",
    "                    t_index = int(np.argmin(np.abs(glac_times - doy_target)))\n",
    "                    closest_date = glac_times[t_index]\n",
    "                    #print(f\"   Closest SAR date: {closest_date}\")\n",
    "\n",
    "                    # Extract the SAR dB slice\n",
    "                    #print(\"glac cube shape:\", glac_cube.shape)\n",
    "                    sar_dB = glac_cube[t_index, :, :]\n",
    "                    #print(\"1::\")\n",
    "                    # print(\"sar_dB shape:\", sar_dB.shape)\n",
    "                    # print(\"mask_good_pixels shape:\", self.mask_good_pixels.shape)\n",
    "                    # Align shapes between SAR slice and mask\n",
    "                    sar_y, sar_x = sar_dB.shape\n",
    "                    mask_y, mask_x = self.glac_mask[glacno].shape\n",
    "\n",
    "                    # Crop mask to match SAR slice dimensions (safe, preserves alignment)\n",
    "                    mask_cropped = self.glac_mask[glacno][:sar_y, :sar_x]\n",
    "                    sar_dB_masked = np.copy(sar_dB)\n",
    "                    sar_dB_masked[mask_cropped != 1] = np.nan\n",
    "                    #print(\"2:\")\n",
    "\n",
    "                    # Create the melt overlay\n",
    "                    # Melted pixels: black overlay, fully opaque where melted == True\n",
    "                    melted_mask_full = np.full_like(self.glac_mask[glacno], np.nan, dtype=float)\n",
    "                    melted_mask_full[:] = np.nan\n",
    "                    melted_mask_full[valid_mask] = melted.astype(float)\n",
    "                    melted_mask_full[melted_mask_full == 0] = np.nan\n",
    "                    #print(\"melted mask shape:\", melted_mask_full.shape)\n",
    "                    #print(\"sar shape:\", sar_dB_masked.shape)\n",
    "\n",
    "                    # --- Plot ---\n",
    "                    # backscatter_cmap = plt.cm.coolwarm_r  # reversed so low values = red, high = blue\n",
    "                                        \n",
    "                    #                     # --- Plot side-by-side ---\n",
    "                    # fig, axes = plt.subplots(1, 2, figsize=(12, 6), constrained_layout=True)\n",
    "\n",
    "                    # # ---------- LEFT: Backscatter + melt overlay ----------\n",
    "                    # ax = axes[0]\n",
    "                    # im0 = ax.imshow(sar_dB_masked, cmap=backscatter_cmap, vmin=-25, vmax=0)\n",
    "                    # ax.imshow(melted_mask_full, cmap=mcolors.ListedColormap([\"black\"]), alpha=0.8)\n",
    "                    # ax.set_title(\n",
    "                    #     f\"{year} DOY {doy}\\n({pd.to_datetime(str(closest_date)).strftime('%Y-%m-%d')}) — Melted pixels (black)\",\n",
    "                    #     fontsize=11\n",
    "                    # )\n",
    "                    # ax.axis(\"off\")\n",
    "\n",
    "                    # # ---------- RIGHT: Raw backscatter only ----------\n",
    "                    # ax = axes[1]\n",
    "                    # im1 = ax.imshow(sar_dB_masked, cmap=backscatter_cmap, vmin=-25, vmax=0)\n",
    "                    # ax.set_title(\"Backscatter (dB)\", fontsize=11)\n",
    "                    # ax.axis(\"off\")\n",
    "\n",
    "                    # # Add one shared colorbar for backscatter\n",
    "                    # cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # (left, bottom, width, height)\n",
    "                    # cbar = fig.colorbar(im1, cax=cbar_ax)\n",
    "                    # cbar.set_label(\"σ⁰ (dB)\", rotation=270, labelpad=15)\n",
    "\n",
    "                    # # Save plot\n",
    "                    # out_path = os.path.join(plot_dir, f\"meltmap_{year}_doy{doy:03d}.png\")\n",
    "                    # plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "                    # plt.close()\n",
    "\n",
    "\n",
    "\n",
    "            # Save CSV\n",
    "            df = pd.DataFrame(results)\n",
    "            min_elev = np.nanmin(self.glac_dem[glacno][self.glac_mask[glacno] == 1])\n",
    "            df[\"melt_extent_elev_m\"].fillna(0.0, inplace=True)\n",
    "            df[\"melt_extent_elev_m\"].replace(0.0, min_elev, inplace=True)\n",
    "\n",
    "            csv_path = os.path.join(out_dir, f\"melt_time_series_{glacno}_{year}.csv\")\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f\"  Saved: {csv_path}\")\n",
    "\n",
    "\n",
    "    def single_glacier_preprocess(self, glacno=None, area_km2=10, verbose=False):\n",
    "        \"\"\"\n",
    "        Glacier melt extent elevations for individual glaciers\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        glacno : int\n",
    "            glacier number within the region (SAR datacube) of interest\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self.data_cp_glac : dictionary of np.arrays\n",
    "            change potential data cubes for each glacier\n",
    "        \"\"\"\n",
    "        self.glac_bounds[glacno] = {}\n",
    "        xmin = np.where(self.mask_values == glacno)[0].min() - 1\n",
    "        xmax = np.where(self.mask_values == glacno)[0].max() + 1\n",
    "        ymin = np.where(self.mask_values == glacno)[1].min() - 1\n",
    "        ymax = np.where(self.mask_values == glacno)[1].max() + 1\n",
    "        self.glac_bounds[glacno]['xmin'] = xmin\n",
    "        self.glac_bounds[glacno]['xmax'] = xmax\n",
    "        self.glac_bounds[glacno]['ymin'] = ymin\n",
    "        self.glac_bounds[glacno]['ymax'] = ymax    \n",
    "        \n",
    "        # Single Glacier Mask\n",
    "        mask_values_glac = self.mask_values[xmin:xmax+1,ymin:ymax+1]\n",
    "        mask_values_binary_nan_glac = np.copy(mask_values_glac).astype(np.float64)\n",
    "        mask_values_binary_nan_glac[mask_values_binary_nan_glac!=glacno] = np.nan\n",
    "        mask_values_binary_nan_glac[mask_values_binary_nan_glac>0] = 1\n",
    "        self.glac_mask[glacno] = mask_values_binary_nan_glac\n",
    "        \n",
    "        # Good Pixel Mask\n",
    "        self.glac_mask_good_pixels[glacno] = self.mask_good_pixels[xmin:xmax+1,ymin:ymax+1]\n",
    "        \n",
    "        # Subset data\n",
    "        self.glac_data[glacno] = self.data_masked[:,xmin:xmax+1,ymin:ymax+1] * self.glac_mask[glacno][np.newaxis,:,:]\n",
    "        self.glac_data_cp[glacno] = self.data_cp[:,xmin:xmax+1,ymin:ymax+1] * self.glac_mask[glacno][np.newaxis,:,:]\n",
    "        self.glac_data_sl_cp[glacno] = self.data_sl_cp[:,xmin:xmax+1,ymin:ymax+1] * self.glac_mask[glacno][np.newaxis,:,:]\n",
    "        \n",
    "        glac_dem = self.dem[xmin:xmax+1,ymin:ymax+1].astype(np.float64)\n",
    "        glac_dem = glac_dem * self.glac_mask[glacno] * self.glac_mask_good_pixels[glacno]\n",
    "        # glac_dem[np.isnan(self.glac_data_cp[glacno][0,:,:])] = np.nan\n",
    "        self.glac_dem[glacno] = glac_dem\n",
    "\n",
    "        # get the elevation of minimum backscatter (for defining maximum snowline cutoff)\n",
    "        glac_data = self.glac_data[glacno] * self.glac_mask[glacno] * self.glac_mask_good_pixels[glacno]\n",
    "        flat_idx = np.nanargmin(glac_data.reshape(glac_data.shape[0], -1), axis=1)\n",
    "        rows, cols = np.unravel_index(flat_idx, glac_data.shape[1:])\n",
    "        min_dB_elevs = glac_dem[rows, cols]\n",
    "        self.min_dB_elevs[glacno] = min_dB_elevs\n",
    "        \n",
    "        # equal elevation bins\n",
    "        bin_min = int(np.floor(np.nanmin(glac_dem) / bin_size) * bin_size)\n",
    "        bin_max = int(np.ceil(np.nanmax(glac_dem) / bin_size) * bin_size)\n",
    "        if verbose:\n",
    "            print('bin_min:', bin_min, '\\nbin_max:', bin_max)\n",
    "        \n",
    "        bins = np.arange(bin_min, bin_max+bin_size, bin_size)\n",
    "        bins_center = np.arange(bin_min + bin_size/2, bin_max, bin_size).astype(int)\n",
    "        bins_count, bins = np.histogram(glac_dem, bins=bins)\n",
    "        nbins = bins_center.shape[0]\n",
    "        \n",
    "        # equal area bins\n",
    "        if self.area_bin_size == 'variable': # if area bin size is variable\n",
    "            min_bins = 100 # get area based on 50 bins\n",
    "            min_bin_size = (area_km2 * 1e6) / min_bins \n",
    "            area_bin_size = np.ceil(min_bin_size / (self.xres * self.yres)) * (self.xres * self.yres)\n",
    "            area_bin_size = max(area_bin_size, 100000) # minimum of 0.1 km2 bins\n",
    "            area_bin_size = min(area_bin_size, 2e6) # maximum of 2 km2 bins\n",
    "            self.area_bin_size = area_bin_size\n",
    "    \n",
    "        assert self.area_bin_size % (self.xres * self.yres) == 0, f'`area_bin_size` is an area not compatible with DEM resolution ({self.xres} m)'\n",
    "        \n",
    "        pixels_per_area_bin = int(self.area_bin_size/(self.xres*self.yres))  # pixels per elevation bin\n",
    "        dem_sort = np.sort(glac_dem[~np.isnan(glac_dem)].flatten()) # remove NaN and sort elevation\n",
    "        \n",
    "        area_bins = dem_sort[::pixels_per_area_bin] # find elevation of bin edges\n",
    "        if area_bins[-1] != dem_sort[-1]:  # include last bin edge\n",
    "            area_bins = np.append(area_bins, dem_sort[-1])\n",
    "        area_bins_center = 0.5 * (area_bins[:-1] + area_bins[1:]) # find bin centers\n",
    "        if verbose:\n",
    "            print('area_bin_min:', area_bins[0], '\\narea_bin_max:', area_bins[-1])\n",
    "\n",
    "        self.glac_bins[glacno] = bins\n",
    "        self.glac_bins_center[glacno] = bins_center\n",
    "        self.glac_area_bins[glacno] = area_bins\n",
    "        self.glac_area_bins_center[glacno] = area_bins_center\n",
    "            \n",
    "        \n",
    "    def melt_elev_percentile_method(self, glacno, csv_fn=None, csv_sl_fn=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Compute Melt Elevations using the Percentile Method\n",
    "\n",
    "        Note: the problem with the percentile method is that ice pixels that are still melting, \n",
    "        but undetected due to the lack of snow prevent the method from using a simple pixel count.  \n",
    "        Hence, this method identifies those pixels based on an \"all melt\" threshold that identfies\n",
    "        pixels melting above them. These pixels are then assumed to be melting. \n",
    "        \n",
    "        \"All-Melt Threshold\"\n",
    "        This threshold is used to identify the elevation at which the bin is melting. \n",
    "        To avoid issues with this being applied too early (e.g., around the ELA where you may have \n",
    "        a mix of ice and firn pixels) this uses a fraction and 100%.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        glacno : int\n",
    "            glacier number within the region (SAR datacube) of interest\n",
    "        verbose : Boolean\n",
    "            print some debugging information or not\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        glac_melt_extent_elevs_percentiles : dictionary of np.arrays\n",
    "            time series of the melt extent elevations for each glacier number\n",
    "        \"\"\"\n",
    "        # Sorted DEM values to use with the percentile method for easy indexing\n",
    "        dem_values_sorted = np.sort(self.glac_dem[glacno].reshape(1,-1))[0,:]\n",
    "        dem_values_sorted = dem_values_sorted[dem_values_sorted > -9999]\n",
    "        \n",
    "        # Process scenes\n",
    "        glac_dem = self.glac_dem[glacno]\n",
    "        data_cp_glac = self.glac_data_cp[glacno]\n",
    "        min_dB_elevs_glac = self.min_dB_elevs[glacno]\n",
    "        melt_extent_elevs, melt_extent_elev_mins, melt_extent_elev_maxs = [], [], []\n",
    "        melt_extent_areas, melt_extent_area_mins, melt_extent_area_maxs = [], [], []\n",
    "        snowline_elevs, snowline_elev_mins, snowline_elev_maxs = [], [], []\n",
    "        snowline_areas, snowline_area_mins, snowline_area_maxs = [], [], []\n",
    "        for nscene in np.arange(data_cp_glac.shape[0]):\n",
    "            data_cp_glac_single = data_cp_glac[nscene,:,:]\n",
    "            min_dB_elevs_glac_single = min_dB_elevs_glac[nscene]\n",
    "        \n",
    "            # Check if all nan values\n",
    "            if len(np.where(~np.isnan(data_cp_glac_single))[0]) == 0:\n",
    "                melt_extent_elevs.append(np.nan)\n",
    "                melt_extent_areas.append(np.nan)\n",
    "        \n",
    "            # Otherwise, calculate extent\n",
    "            else:\n",
    "                # ---------- ---------- ALL MELT CORRECTION: equal elevation bins ---------- ----------\n",
    "                allmelt_elev = 0\n",
    "                allmelt_100 = False\n",
    "                for nbin, bin_elev_lower in enumerate(self.glac_bins[glacno][:-1]):\n",
    "                    bin_elev_upper = self.glac_bins[glacno][nbin+1]\n",
    "                    if not allmelt_100:\n",
    "                        # Create mask based on elevations\n",
    "                        mask_bin = np.zeros(glac_dem.shape)\n",
    "                        mask_bin[(glac_dem > bin_elev_lower) & (glac_dem <= bin_elev_upper)] = 1\n",
    "                        bin_count = mask_bin.sum()\n",
    "                \n",
    "                        data_cp_bin = data_cp_glac_single * mask_bin\n",
    "                        data_cp_bin_count = np.nansum(data_cp_bin)\n",
    "                \n",
    "                        frac_melt = data_cp_bin_count / bin_count\n",
    "                \n",
    "                        # Record \"all melt\" elevation \n",
    "                        if allmelt_elev == 0 and frac_melt > allmelt_threshold and bin_count > allmelt_pixels:\n",
    "                            allmelt_elev = bin_elev_lower\n",
    "                        # Record \"all melt\" elevation in the case that 100% hasn't been found yet\n",
    "                        if not allmelt_100 and frac_melt == 1:\n",
    "                            allmelt_elev = bin_elev_lower\n",
    "                            allmelt_100 = True\n",
    "\n",
    "                # Apply \"all melt\" correction\n",
    "                if allmelt_elev > 0:\n",
    "                    data_cp_glac_single[glac_dem < allmelt_elev] = 1\n",
    "            \n",
    "                # ----- PERCENTILE METHOD -----\n",
    "                melt_pixels = np.nansum(data_cp_glac_single)\n",
    "        \n",
    "                # The index is associated with one less than the sum of the pixels to account for indexing starting with 0 not 1\n",
    "                if melt_pixels == 0:\n",
    "                    melt_idx = int(0)\n",
    "                else:\n",
    "                    melt_idx = int(melt_pixels - 1)\n",
    "\n",
    "                melt_extent_elev = dem_values_sorted[melt_idx]\n",
    "                melt_extent_elevs.append(melt_extent_elev)\n",
    "                \n",
    "                # percentile method uncertainty\n",
    "                nomelt_pixels_below = np.nansum((glac_dem < melt_extent_elev) & (data_cp_glac_single == 0))\n",
    "                melt_pixels_above = np.nansum((glac_dem > melt_extent_elev) & (data_cp_glac_single == 1))\n",
    "                melt_extent_elev_min = dem_values_sorted[melt_idx - nomelt_pixels_below]\n",
    "                melt_extent_elev_max = dem_values_sorted[melt_idx + melt_pixels_above]\n",
    "                melt_extent_elev_mins.append(melt_extent_elev_min)\n",
    "                melt_extent_elev_maxs.append(melt_extent_elev_max)\n",
    "\n",
    "                # ---------- ---------- ALL MELT CORRECTION: repeat for equal area elevation bin ---------- ----------\n",
    "                allmelt_elev = 0\n",
    "                allmelt_100 = False\n",
    "                for nbin, bin_elev_lower in enumerate(self.glac_area_bins[glacno][:-1]):\n",
    "                    bin_elev_upper = self.glac_area_bins[glacno][nbin+1]\n",
    "                    if not allmelt_100:\n",
    "                        # Create mask based on elevations\n",
    "                        mask_bin = np.zeros(glac_dem.shape)\n",
    "                        mask_bin[(glac_dem > bin_elev_lower) & (glac_dem <= bin_elev_upper)] = 1\n",
    "                        bin_count = mask_bin.sum()\n",
    "                \n",
    "                        data_cp_bin = data_cp_glac_single * mask_bin\n",
    "                        data_cp_bin_count = np.nansum(data_cp_bin)\n",
    "                \n",
    "                        frac_melt = data_cp_bin_count / bin_count\n",
    "                \n",
    "                        # Record \"all melt\" elevation \n",
    "                        if allmelt_elev == 0 and frac_melt > allmelt_threshold and bin_count > allmelt_pixels:\n",
    "                            allmelt_elev = bin_elev_lower\n",
    "                        # Record \"all melt\" elevation in the case that 100% hasn't been found yet\n",
    "                        if not allmelt_100 and frac_melt == 1:\n",
    "                            allmelt_elev = bin_elev_lower\n",
    "                            allmelt_100 = True\n",
    "\n",
    "                # Apply \"all melt\" correction\n",
    "                if allmelt_elev > 0:\n",
    "                    data_cp_glac_single[glac_dem < allmelt_elev] = 1\n",
    "            \n",
    "                # ----- PERCENTILE METHOD -----\n",
    "                melt_pixels = np.nansum(data_cp_glac_single)\n",
    "        \n",
    "                # The index is associated with one less than the sum of the pixels to account for indexing starting with 0 not 1\n",
    "                if melt_pixels == 0:\n",
    "                    melt_idx = int(0)\n",
    "                else:\n",
    "                    melt_idx = int(melt_pixels - 1)               \n",
    "                melt_extent_area = melt_idx*self.xres*self.yres\n",
    "                melt_extent_areas.append(melt_extent_area)\n",
    "\n",
    "                # percentile method uncertainty\n",
    "                nomelt_pixels_below = np.nansum((glac_dem < dem_values_sorted[melt_idx]) & (data_cp_glac_single == 0))\n",
    "                melt_pixels_above = np.nansum((glac_dem > dem_values_sorted[melt_idx]) & (data_cp_glac_single == 1))\n",
    "                melt_extent_area_min = (melt_idx - nomelt_pixels_below)*self.xres*self.yres\n",
    "                melt_extent_area_max = (melt_idx + melt_pixels_above)*self.xres*self.yres\n",
    "                melt_extent_area_mins.append(melt_extent_area_min)\n",
    "                melt_extent_area_maxs.append(melt_extent_area_max)\n",
    "\n",
    "                # ------------------------- SNOWLINES ------------------------------\n",
    "                # SNOWLINES: cp values that are back to 0 below the melt extent (minimum to be conservative) or elevation of min backscatter\n",
    "                data_sl_cp_glac = self.glac_data_sl_cp[glacno]\n",
    "                data_sl_cp_glac_single = data_sl_cp_glac[nscene,:,:]\n",
    "                data_sl_elev_max = min(min_dB_elevs_glac_single, melt_extent_elev_min)\n",
    "\n",
    "                snowline_cp = (data_sl_cp_glac_single == 1) & (glac_dem < data_sl_elev_max)\n",
    "                snowline_idx = max(np.nansum(snowline_cp) - 1, 0)\n",
    "                \n",
    "                snowline_elev = dem_values_sorted[snowline_idx]\n",
    "                snowline_area = snowline_idx*self.xres*self.yres\n",
    "\n",
    "                # percentile method uncertainty for snowlines\n",
    "                # snow_pixels_below = np.nansum((glac_dem < snowline_elev) & (data_sl_cp_glac_single == 0))\n",
    "                # nosnow_pixels_above = np.nansum((glac_dem > snowline_elev) & (data_sl_cp_glac_single == 1) & \n",
    "                #                                 (glac_dem < melt_extent_elev_min))\n",
    "                snow_pixels_below = (np.nansum((glac_dem < snowline_elev) & (data_sl_cp_glac_single == 0)) + \n",
    "                                     np.nansum((glac_dem < snowline_elev) & self.glac_mask[glacno].astype(bool) & ~self.glac_mask_good_pixels[glacno].astype(bool)))\n",
    "                nosnow_pixels_above = (np.nansum((glac_dem > snowline_elev) & (data_sl_cp_glac_single == 1) & (glac_dem < melt_extent_elev_min)) +\n",
    "                                       np.nansum((glac_dem > snowline_elev) & self.glac_mask[glacno].astype(bool) & ~self.glac_mask_good_pixels[glacno].astype(bool)))\n",
    "\n",
    "                snowline_elev_min = dem_values_sorted[snowline_idx - snow_pixels_below]\n",
    "                snowline_elev_max = dem_values_sorted[snowline_idx + nosnow_pixels_above]\n",
    "                snowline_area_min = (snowline_idx - snow_pixels_below)*self.xres*self.yres\n",
    "                snowline_area_max = (snowline_idx + nosnow_pixels_above)*self.xres*self.yres\n",
    "\n",
    "                # add to lists\n",
    "                snowline_elevs.append(snowline_elev)\n",
    "                snowline_areas.append(snowline_area)\n",
    "                snowline_elev_mins.append(snowline_elev_min)\n",
    "                snowline_elev_maxs.append(snowline_elev_max)\n",
    "                snowline_area_mins.append(snowline_area_min)\n",
    "                snowline_area_maxs.append(snowline_area_max)\n",
    "\n",
    "        \n",
    "        self.glac_melt_extent_elevs_percentiles[glacno] = np.array(melt_extent_elevs)\n",
    "        self.glac_melt_extent_elevs_percentile_mins[glacno] = np.array(melt_extent_elev_mins)\n",
    "        self.glac_melt_extent_elevs_percentile_maxs[glacno] = np.array(melt_extent_elev_maxs)\n",
    "        self.glac_melt_extent_areas_percentiles[glacno] = np.array(melt_extent_areas)\n",
    "        self.glac_melt_extent_areas_percentile_mins[glacno] = np.array(melt_extent_area_mins)\n",
    "        self.glac_melt_extent_areas_percentile_maxs[glacno] = np.array(melt_extent_area_maxs)\n",
    "\n",
    "        self.glac_snowline_elevs_percentiles[glacno] = np.array(snowline_elevs)\n",
    "        self.glac_snowline_elevs_percentile_mins[glacno] = np.array(snowline_elev_mins)\n",
    "        self.glac_snowline_elevs_percentile_maxs[glacno] = np.array(snowline_elev_maxs)\n",
    "        self.glac_snowline_areas_percentiles[glacno] = np.array(snowline_areas)\n",
    "        self.glac_snowline_areas_percentile_mins[glacno] = np.array(snowline_area_mins)\n",
    "        self.glac_snowline_areas_percentile_maxs[glacno] = np.array(snowline_area_maxs)\n",
    "        \n",
    "        # Export binned data        \n",
    "        if not csv_fn is None:\n",
    "            me_df = pd.DataFrame(self.glac_melt_extent_elevs_percentiles[glacno], columns=['melt_extent_elev_m'], index=self.dates)\n",
    "            me_df['melt_extent_elev_min_m'] = self.glac_melt_extent_elevs_percentile_mins[glacno]\n",
    "            me_df['melt_extent_elev_max_m'] = self.glac_melt_extent_elevs_percentile_maxs[glacno]\n",
    "            me_df['melt_extent_elev_diff_mean_m'] = ((me_df['melt_extent_elev_max_m'] - me_df['melt_extent_elev_m']) +\n",
    "                                                     (me_df['melt_extent_elev_m'] - me_df['melt_extent_elev_min_m']))/2\n",
    "            me_df.to_csv(csv_fn)\n",
    "            \n",
    "            me_df = pd.DataFrame(self.glac_melt_extent_areas_percentiles[glacno], columns=['melt_extent_area_m2'], index=self.dates)\n",
    "            me_df['melt_extent_area_min_m2'] = self.glac_melt_extent_areas_percentile_mins[glacno]\n",
    "            me_df['melt_extent_area_max_m2'] = self.glac_melt_extent_areas_percentile_maxs[glacno]\n",
    "            me_df['melt_extent_area_diff_mean_m2'] = ((me_df['melt_extent_area_max_m2'] - me_df['melt_extent_area_m2']) +\n",
    "                                                      (me_df['melt_extent_area_m2'] - me_df['melt_extent_area_min_m2']))/2\n",
    "            me_df.to_csv(csv_fn[:-4]+'_eabin.csv')\n",
    "        else:\n",
    "            return (self.dates, self.glac_melt_extent_elevs_percentiles[glacno], self.glac_melt_extent_areas_percentiles[glacno], \n",
    "                    self.glac_snowline_elevs_percentiles[glacno], self.glac_snowline_areas_percentiles[glacno])\n",
    "\n",
    "        if not csv_sl_fn is None:\n",
    "            sl_df = pd.DataFrame(self.glac_snowline_elevs_percentiles[glacno], columns=['snowline_elev_m'], index=self.dates)\n",
    "            sl_df['snowline_elev_min_m'] = self.glac_snowline_elevs_percentile_mins[glacno]\n",
    "            sl_df['snowline_elev_max_m'] = self.glac_snowline_elevs_percentile_maxs[glacno]\n",
    "            sl_df['snowline_elev_diff_mean_m'] = ((sl_df['snowline_elev_max_m'] - sl_df['snowline_elev_m']) +\n",
    "                                                  (sl_df['snowline_elev_m'] - sl_df['snowline_elev_min_m']))/2\n",
    "            sl_df.to_csv(csv_sl_fn)\n",
    "            \n",
    "            sl_df = pd.DataFrame(self.glac_snowline_areas_percentiles[glacno], columns=['snowline_area_m2'], index=self.dates)\n",
    "            sl_df['snowline_area_min_m2'] = self.glac_snowline_areas_percentile_mins[glacno]\n",
    "            sl_df['snowline_area_max_m2'] = self.glac_snowline_areas_percentile_maxs[glacno]\n",
    "            sl_df['snowline_area_diff_mean_m2'] = ((sl_df['snowline_area_max_m2'] - sl_df['snowline_area_m2']) +\n",
    "                                                   (sl_df['snowline_area_m2'] - sl_df['snowline_area_min_m2']))/2\n",
    "            sl_df.to_csv(csv_sl_fn[:-4]+'_eabin.csv')\n",
    "\n",
    "    \n",
    "    def db_heatmap(self, glacno, csv_fn=None, hyps_fn=None):\n",
    "        \"\"\"\n",
    "        Bin backscatter to produce heatmaps\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        glacno : int\n",
    "            glacier number within the region (SAR datacube) of interest\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        glac_binned_db : dictionary of np.arrays\n",
    "            binned backscatter for each glacier number\n",
    "        \"\"\"\n",
    "        for i in [0, 1]:\n",
    "            if i == 0:\n",
    "                bins = self.glac_bins[glacno]\n",
    "                bins_center = self.glac_bins_center[glacno]\n",
    "            else:\n",
    "                bins = self.glac_area_bins[glacno]\n",
    "                bins_center = self.glac_area_bins_center[glacno]\n",
    "            \n",
    "            glac_dem = self.glac_dem[glacno]\n",
    "            glac_data = self.glac_data[glacno]\n",
    "            \n",
    "            db_bin = np.zeros((len(bins)-1, glac_data.shape[0]))\n",
    "            db_bin[:,:] = np.nan\n",
    "            binned_pixels = np.zeros((len(bins)-1))\n",
    "            for nbin, bin_elev_lower in enumerate(bins[:-1]):\n",
    "                # Create mask based on elevations\n",
    "                bin_elev_upper = bins[nbin+1]\n",
    "                mask_bin = np.zeros(glac_dem.shape)\n",
    "                mask_bin[(glac_dem > bin_elev_lower) & (glac_dem <= bin_elev_upper)] = 1\n",
    "                bin_pixels = mask_bin.sum()\n",
    "            \n",
    "                # Mask pixels\n",
    "                data_glac_singlebin = glac_data * mask_bin[np.newaxis,:,:]\n",
    "            \n",
    "                # Manually average based on summing and pixel counts\n",
    "                #  note: this avoids masking the entire data stack which is slow\n",
    "                bin_sar_series = np.nansum(data_glac_singlebin, axis=(1, 2)) / bin_pixels\n",
    "                bin_sar_series[bin_sar_series == 0] = np.nan\n",
    "                db_bin[nbin,:] = bin_sar_series\n",
    "            \n",
    "                binned_pixels[nbin] = bin_pixels\n",
    "    \n",
    "            # Export binned data\n",
    "            if not csv_fn is None:\n",
    "                db_bin_df = pd.DataFrame(db_bin, columns=self.dates, index=bins_center)\n",
    "                if i == 0:\n",
    "                    db_bin_df.to_csv(csv_fn)\n",
    "                else:\n",
    "                    db_bin_df.to_csv(csv_fn[:-4]+'_eabin.csv')\n",
    "\n",
    "            # Export hypsometry\n",
    "            binned_area = binned_pixels * self.xres * self.yres\n",
    "            if not hyps_fn is None:\n",
    "                hyps_df = pd.DataFrame(binned_area, columns=['area_m2'], index=bins_center)\n",
    "                hyps_df.index.name = 'Elev_m'\n",
    "                if i == 0:\n",
    "                    hyps_df.to_csv(hyps_fn)\n",
    "                else:\n",
    "                    hyps_df.to_csv(hyps_fn[:-4]+'_eabin.csv')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0d690-ec5d-4fb4-9686-88740136e194",
   "metadata": {},
   "source": [
    "Main function to process data and extract melt extents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f47dbc2-0951-4f31-a6ce-7d71cf2300cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '_csv/'  # replace with your folder path\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af4f1b-e8e6-4180-9bf6-db99e51a09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spatial_subsets(ny, nx, tile_y=500, tile_x=500):\n",
    "    \"\"\"\n",
    "    Yields (subset_y, subset_x, y0, y1, x0, x1) covering the spatial domain.\n",
    "    \"\"\"\n",
    "    for y0 in range(0, ny, tile_y):\n",
    "        y1 = min(y0 + tile_y, ny)\n",
    "        for x0 in range(0, nx, tile_x):\n",
    "            x1 = min(x0 + tile_x, nx)\n",
    "            yield slice(y0, y1), slice(x0, x1), y0, y1, x0, x1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_datacube_to_melt_extent(i, outer_rng, desc, ds_fns, scene_name, rgi_reg, xres, yres, min_glac_area_km2, db_threshold, \n",
    "                                    db_threshold_sl, zscore_threshold, winter_months, snowmelt_months, months2exclude_cp, \n",
    "                                    winter_std_threshold, bin_size, area_bin_size, allmelt_threshold, \n",
    "                                    allmelt_pixels, verbose=True):\n",
    "    # Process Datasets\n",
    "    pathrow_strs = []\n",
    "    failed_glacnos = []\n",
    "    for ds_fn in ds_fns:\n",
    "        # Path/Row string used for filenames\n",
    "        pathrow_str = str(ds_fn).split('.nc')[0].split(pol_str)[1][1:]\n",
    "\n",
    "        with xr.open_dataset(ds_fn) as tmp_ds:\n",
    "            ny = tmp_ds.sizes['y']\n",
    "            nx = tmp_ds.sizes['x']\n",
    "\n",
    "        tile_y, tile_x = 500, 500\n",
    "\n",
    "        for subset_y, subset_x, y0, y1, x0, x1 in generate_spatial_subsets(\n",
    "            ny, nx, tile_y=tile_y, tile_x=tile_x):\n",
    "\n",
    "            print(f\"\\n=== ds_fn={ds_fn}, tile y[{y0}:{y1}], x[{x0}:{x1}] ===\")\n",
    "            \n",
    "            # Initialize the datacube (this is slow because we're loading/opening large datafiles)\n",
    "            dc = sar_datacube(ds_fn, \n",
    "                            scene_name=scene_name,\n",
    "                            rgi_reg=rgi_reg,\n",
    "                            xres=xres,\n",
    "                            yres=yres,\n",
    "                            min_glac_area_km2=min_glac_area_km2,\n",
    "                            db_threshold=db_threshold,\n",
    "                            db_threshold_sl=db_threshold_sl,\n",
    "                            zscore_threshold=zscore_threshold,\n",
    "                            winter_months=winter_months,\n",
    "                            snowmelt_months=snowmelt_months,\n",
    "                            months2exclude_cp=months2exclude_cp,\n",
    "                            winter_std_threshold=winter_std_threshold,\n",
    "                            bin_size=bin_size,\n",
    "                            area_bin_size=area_bin_size,\n",
    "                            allmelt_threshold=allmelt_threshold,\n",
    "                            allmelt_pixels=allmelt_pixels,\n",
    "                            subset_y=subset_y,\n",
    "                            subset_x=subset_x,)\n",
    "        \n",
    "            # Load glaciers to process\n",
    "            main_glac_rgi = dc.glacnos_to_process()\n",
    "        \n",
    "            # Add to RGI Path/Row Dictionary\n",
    "            for rgino_str in main_glac_rgi.rgino_str.values:\n",
    "                if not rgino_str in rgino_ds_dict.keys():\n",
    "                    rgino_ds_dict[rgino_str] = [ds_fn]\n",
    "                elif not ds_fn in rgino_ds_dict[rgino_str]:\n",
    "                    rgino_ds_dict[rgino_str].append(ds_fn)\n",
    "        \n",
    "            # convert PosixPath objects to strings\n",
    "            with open(rgi_ds_fn, 'w') as json_file:\n",
    "                json.dump(rgino_ds_dict, json_file, indent=4)\n",
    "        \n",
    "            # Remove pixels from non-glaciated areas\n",
    "            dc.mask_nonglacier_pixels(main_glac_rgi)\n",
    "        \n",
    "            # PIXEL-BY-PIXEL ANALYSIS\n",
    "            dc.pixel_analysis()\n",
    "            dc.annual_melt_onset_map()\n",
    "            dc.generate_snowline_post_onset_mask()\n",
    "            dc.annual_snowline_post_onset_map()\n",
    "            #dc.plot_snowline_vs_melt_onsets()\n",
    "            #dc.print_snowline_melt_difference_diagnostics()\n",
    "\n",
    "            out_dir = os.path.join(os.getcwd(), 'onsets')\n",
    "            dc.save_melt_onset_tiffs(out_dir)\n",
    "            dc.save_snowmelt_onset_tiffs(out_dir)\n",
    "\n",
    "            \n",
    "            \n",
    "            #dc.diagnose_snowline_activity()\n",
    "        \n",
    "            # SINGLE GLACIER ANALYSIS\n",
    "            for nglac, glacno in enumerate(tqdm(main_glac_rgi.glacno.values, file=desc, \n",
    "                                                desc=f'Processing {len(main_glac_rgi)} glaciers in path_row {pathrow_str} (data_no: {i})')):\n",
    "            #for nglac, glacno in enumerate([6025]): # or specify a particular glacier\n",
    "                outer_rng.set_description(desc.read())\n",
    "                \n",
    "                # RGI_str for filenames\n",
    "                nidx = list(main_glac_rgi.glacno.values).index(glacno)\n",
    "                rgino_str = main_glac_rgi.loc[nidx,'rgino_str']\n",
    "                area_km2 = main_glac_rgi.loc[nidx,'area_km2']\n",
    "        \n",
    "                #try:\n",
    "                # Process individual glacier\n",
    "                dc.area_bin_size = area_bin_size\n",
    "                dc.single_glacier_preprocess(glacno=glacno, area_km2=area_km2)\n",
    "\n",
    "                out_dir = os.path.join(os.getcwd(), 'csv_me_tests')\n",
    "                dc.generate_melt_extent_elevations_from_onset(glacno, \n",
    "                                                    out_dir,\n",
    "                                                    doy_step=10,\n",
    "                                                    percentile=1.0,\n",
    "                                                    min_valid_frac=0.01)\n",
    "                \n",
    "                # Melt Elevation Extents\n",
    "                melt_extent_perc_fn = csv_fp + rgino_str + '_melt_extent_elev_percentile_' + pathrow_str + '.csv'\n",
    "                cwd = os.getcwd()\n",
    "                csv_folder = os.path.join(cwd, csv_fp)\n",
    "                melt_extent_perc_fn = os.path.join(csv_folder, (rgino_str + '_melt_extent_elev_percentile_' + pathrow_str + '.csv'))\n",
    "                snowline_perc_fn = os.path.join(csv_folder, (rgino_str + '_snowline_elev_percentile_' + pathrow_str + '.csv'))\n",
    "                dc.melt_elev_percentile_method(glacno=glacno, csv_fn=melt_extent_perc_fn, csv_sl_fn=snowline_perc_fn)\n",
    "                \n",
    "                # Binned Heatmap: Export CSV file (medium)\n",
    "                db_binned_csv_fn = os.path.join(csv_folder, (rgino_str + '_db_bin_mean_' + pathrow_str + '.csv'))\n",
    "                hyps_fn = os.path.join(csv_folder, (rgino_str + '_hypsometry_' + pathrow_str + '.csv')) \n",
    "                dc.db_heatmap(glacno=glacno, csv_fn=db_binned_csv_fn, hyps_fn=hyps_fn)\n",
    "                \n",
    "            \n",
    "                # except:\n",
    "                #     failed_glacnos.append(glacno)\n",
    "            \n",
    "            return len(Dataset(ds_fns[0], mode=\"r\").variables[\"time\"]), failed_glacnos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ba7d0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\csv\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "csv_folder = os.path.join(cwd, 'csv')\n",
    "print(csv_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d68d8-a59e-4f47-b45c-8e682a5a6f9c",
   "metadata": {},
   "source": [
    "# PROCESS GLACIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c579559-a71b-43ba-97b7-12a4d139ec47",
   "metadata": {},
   "source": [
    "#### Input information for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22e07339-141c-4365-be57-c20a2007f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_reg = 1            # RGI region\n",
    "min_glac_area_km2 = 2  # Minimum glacier area to process [sq.km]\n",
    "db_threshold = -3      # Threshold for drop in db relative to winter mean to identify melt\n",
    "db_threshold_sl=4      # Threshold for drop in db relative to summer minimum backscatter to identify snowlines\n",
    "zscore_threshold = -2  # Threshold for z-score to ensure the drop in backscatter exceeds the variance of winter pixels\n",
    "winter_months = [1,2]  # Months to use to estimate winter months (using Jan/Feb helps avoid rain-on-snow in shoulder seasons)\n",
    "snowmelt_months = [4,5,6,7] # Months to use to estimate snowmelt months for minimum backscatter pixels\n",
    "months2exclude_cp = [10,11,12,1,2] # Month to exclude from change pixel timing\n",
    "winter_std_threshold = 3 # winter month standard deviation threshold [dB] (if a pixel has a larger std than this, it is remove from analysis)\n",
    "\n",
    "# recommended bin sizes per type\n",
    "bin_size = 20 # elevation bin sizes in meters\n",
    "area_bin_size = 'variable' # equal-area bin sizes in square meters\n",
    "allmelt_threshold = 0.9 # Threshold above which assume elevations below are melting (deals with clean ice/debris appearing not to melt)\n",
    "allmelt_pixels = 10 # minimum number of pixels to use the allmelt_threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa6163-2c8b-46c5-966f-aa5427d587fc",
   "metadata": {},
   "source": [
    "#### Get filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a927be06-e1d4-414b-b836-1d8beaa6116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a path/frame\n",
    "data_no = str(1)\n",
    "# imagedirsPath = loaded_sar_data[data_no]['imagedirsPath']\n",
    "# location_str = loaded_sar_data[data_no]['location_str']\n",
    "# scene_name = loaded_sar_data[data_no]['scene_name']\n",
    "# epsg_no = loaded_sar_data[data_no]['epsg_no']\n",
    "# path_frame_dict = loaded_sar_data[data_no]['path_frame_dict']\n",
    "# path_direction = loaded_sar_data[data_no]['Direction']\n",
    "# frame_cut = loaded_sar_data[data_no]['frame_cut']\n",
    "# loaded_sar_data[data_no]\n",
    "location_str = 'kennicott'\n",
    "imagedirsPath = None\n",
    "scene_name = 'Kennicott'\n",
    "epsg_no = 32607\n",
    "path_frame_dict = {'14': ['387']}\n",
    "path_direction = 'Ascending'\n",
    "frame_cut = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56567bf7-203b-4ea8-88a7-ecca5d0d30db",
   "metadata": {},
   "source": [
    "#### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6c8a354d-3de1-46d7-819c-95630b2c4788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[0:500], x[0:500] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[0:500], x[500:1000] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[0:500], x[1000:1500] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[0:500], x[1500:2000] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[0:500], x[2000:2500] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[0:500], x[2500:2921] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[500:1000], x[0:500] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[500:1000], x[500:1000] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[500:1000], x[1000:1500] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[500:1000], x[1500:2000] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[500:1000], x[2000:2500] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[500:1000], x[2500:2921] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[1000:1500], x[0:500] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[1000:1500], x[500:1000] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[1000:1500], x[1000:1500] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[1000:1500], x[1500:2000] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[1000:1500], x[2000:2500] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[1000:1500], x[2500:2921] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[1500:2000], x[0:500] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[1500:2000], x[500:1000] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[1500:2000], x[1000:1500] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[1500:2000], x[1500:2000] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[1500:2000], x[2000:2500] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[1500:2000], x[2500:2921] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[2000:2225], x[0:500] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[2000:2225], x[500:1000] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[2000:2225], x[1000:1500] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[2000:2225], x[1500:2000] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[2000:2225], x[2000:2500] ===\n",
      "\n",
      "=== ds_fn=c:\\Users\\jaden\\Downloads\\Research\\onset_maps\\Kennicott_32607_S1_cube_VH_014_387.nc, tile y[2000:2225], x[2500:2921] ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     47\u001b[39m     rgino_ds_dict = {}\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m scenes_no, failed_glacnos = process_datacube_to_melt_extent(i=data_no, outer_rng=outer_rng, desc=desc, ds_fns=ds_fns, \n\u001b[32m     50\u001b[39m         scene_name=scene_name, rgi_reg=rgi_reg, xres=xres, yres=yres, min_glac_area_km2=min_glac_area_km2, \n\u001b[32m     51\u001b[39m         db_threshold=db_threshold, db_threshold_sl=db_threshold_sl, zscore_threshold=zscore_threshold, winter_months=winter_months, \n\u001b[32m     52\u001b[39m         snowmelt_months=snowmelt_months, months2exclude_cp=months2exclude_cp, winter_std_threshold=winter_std_threshold, \n\u001b[32m     53\u001b[39m         bin_size=bin_size, area_bin_size=area_bin_size, allmelt_threshold=allmelt_threshold, allmelt_pixels=allmelt_pixels)\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mscenes:\u001b[39m\u001b[33m'\u001b[39m, scenes_no)\n\u001b[32m     55\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mfailed glaciers:\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(failed_glacnos), failed_glacnos, \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "data_nos = [1] # process all data from this path-frame\n",
    "\n",
    "outer_rng = tqdm(data_nos)\n",
    "desc=DescStr()\n",
    "for data_no in outer_rng:\n",
    "    # load data\n",
    "    # imagedirsPath = loaded_sar_data[str(data_no)]['imagedirsPath']\n",
    "    # location_str = loaded_sar_data[str(data_no)]['location_str']\n",
    "    # scene_name = loaded_sar_data[str(data_no)]['scene_name']\n",
    "    # epsg_no = loaded_sar_data[str(data_no)]['epsg_no']\n",
    "    # path_frame_dict = loaded_sar_data[str(data_no)]['path_frame_dict']\n",
    "    # path_direction = loaded_sar_data[str(data_no)]['Direction']\n",
    "    # frame_cut = loaded_sar_data[str(data_no)]['frame_cut']\n",
    "    # loaded_sar_data[str(data_no)]\n",
    "\n",
    "    location_str = 'kennicott-Cordoba'\n",
    "    imagedirsPath = None\n",
    "    scene_name = 'Kennicott-Cordoba'\n",
    "    epsg_no = 32607\n",
    "    path_frame_dict = {'14': ['387']}\n",
    "    path_direction = 'Ascending'\n",
    "    frame_cut = 0\n",
    "\n",
    "    # set up output nc file name start (include epsg code in name)\n",
    "    out_nc_filename_start = f'{location_str}_{epsg_no}_S1_cube'\n",
    "    pol_str = 'VH' # can also be 'HV' depending on your data!\n",
    "    \n",
    "    # dictionary to contain list of images for each date for each path (could be one image per date, or a pair along orbit from that date)\n",
    "    indate_and_zips_dict = {x:[] for x in path_frame_dict.keys()}\n",
    "    ds_fns = []\n",
    "    # for path in indate_and_zips_dict.keys():\n",
    "    #     indate_and_zips = indate_and_zips_dict[path]\n",
    "    \n",
    "    #     out_nc_filename = str(out_nc) + f'/{out_nc_filename_start}_{pol_str}_{path}_{\"_\".join(path_frame_dict[path])}.nc'\n",
    "    #     ds_fns.append(out_nc_filename)\n",
    "\n",
    "    out_nc_filename = os.path.join(os.getcwd(), \"Kennicott_32607_S1_cube_VH_014_387.nc\")\n",
    "    ds_fns.append(out_nc_filename)\n",
    "    \n",
    "    # RGI ID vs path-row dictionary\n",
    "    # makes it easier to access individual runs later on; code simply appends the path/row scenes for each glacier within a dictionary\n",
    "    rgi_ds_fn = 'rgino_ds_dict.json'\n",
    "    if os.path.exists(rgi_ds_fn):\n",
    "        with open(rgi_ds_fn, 'r') as json_file:\n",
    "            rgino_ds_dict = json.load(json_file)\n",
    "    else:\n",
    "        rgino_ds_dict = {}\n",
    "    \n",
    "    scenes_no, failed_glacnos = process_datacube_to_melt_extent(i=data_no, outer_rng=outer_rng, desc=desc, ds_fns=ds_fns, \n",
    "            scene_name=scene_name, rgi_reg=rgi_reg, xres=xres, yres=yres, min_glac_area_km2=min_glac_area_km2, \n",
    "            db_threshold=db_threshold, db_threshold_sl=db_threshold_sl, zscore_threshold=zscore_threshold, winter_months=winter_months, \n",
    "            snowmelt_months=snowmelt_months, months2exclude_cp=months2exclude_cp, winter_std_threshold=winter_std_threshold, \n",
    "            bin_size=bin_size, area_bin_size=area_bin_size, allmelt_threshold=allmelt_threshold, allmelt_pixels=allmelt_pixels)\n",
    "    print('scenes:', scenes_no)\n",
    "    print('failed glaciers:', len(failed_glacnos), failed_glacnos, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbee761-c4a7-4a16-97dd-791d2a25383f",
   "metadata": {},
   "source": [
    "Reorganize files into a folder for each glacier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5168acf6-856e-40f5-a03b-3cc4b580f26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code to reorganize csv files into folders\n",
    "import shutil\n",
    "\n",
    "# get all files matching the pattern \"01.XXXXX*\"\n",
    "main_directory = os.getcwd()\n",
    "csv_fp = main_directory + '/csv/'\n",
    "csv_files = glob.glob(os.path.join(csv_fp, f\"{str(rgi_reg).zfill(2)}.[0-9]*.csv\"))\n",
    "\n",
    "\n",
    "# Process each file\n",
    "for csv_file in csv_files:\n",
    "    csv_fn = os.path.basename(csv_file)\n",
    "    glacier_name = csv_fn[:8] # extract glacier name (first 8 characters: \"01.XXXXX\")\n",
    "\n",
    "    glac_folder = os.path.join(csv_fp, glacier_name) # create the target directory if it doesn't exist\n",
    "    os.makedirs(glac_folder, exist_ok=True)\n",
    "\n",
    "    shutil.move(csv_file, os.path.join(glac_folder, csv_fn)) # move the file to its corresponding glacier folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31196c-9881-4481-81f9-28d4b06f40be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcec53a9-a199-4e73-9d3b-a6d05da84afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c407e50-45cc-474b-8c46-d48313880325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167171f7-e328-4a4e-8abf-f66d36a898fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2b63e1-7818-4386-b99a-11231d996423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840063ff-c317-4235-9b71-6c6d11330a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e25ee64-a57d-41c2-99c8-6cdd7c37a4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glacier-env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
